{'dependent': 'kerasrl', 'repo': '/Nikolay-Lysenko/rl-musician', 'language': 'Python', 'description': 'Composition of short musical pieces with reinforcement learning. To be more detailed, framing of first species counterpoint writing as an optimization problem.', 'readme': "[![Build Status](https://travis-ci.org/Nikolay-Lysenko/rl-musician.svg?branch=master)](https://travis-ci.org/Nikolay-Lysenko/rl-musician)\n[![codecov](https://codecov.io/gh/Nikolay-Lysenko/rl-musician/branch/master/graph/badge.svg)](https://codecov.io/gh/Nikolay-Lysenko/rl-musician)\n[![Maintainability](https://api.codeclimate.com/v1/badges/a43618b5f9454d01186c/maintainability)](https://codeclimate.com/github/Nikolay-Lysenko/rl-musician/maintainability)\n[![PyPI version](https://badge.fury.io/py/rl-musician.svg)](https://badge.fury.io/py/rl-musician)\n\n# RL-Musician\n\n## Overview\n\nAs of now, this is a proof-of-concept for music composition with reinforcement learning solely. Here, an agent writes [first species counterpoint](https://en.wikipedia.org/wiki/Counterpoint#Species_counterpoint) and environment is based on a special data structure that represents musical piece. An action is filling of current [measure](https://en.wikipedia.org/wiki/Bar_\\(music\\)) for all melodic lines simultaneously, an episode is finished when all measures are filled one by one, and reward is determined by applying evaluation rules to the resulting piece.\n\nComparing to music composition tools such as [MuseNet](https://openai.com/blog/musenet/), reinforcement learning approach (at least in theory) has two advantages:\n* Actual creativity lies not in imitation of famous pieces, but in finding new ways to create something to be called art. Reinforcement learning meets this criterion, but supervised learning doesn't.\n* There are [tuning systems](https://en.wikipedia.org/wiki/Musical_tuning#Tuning_systems) other than [equal temperament](https://en.wikipedia.org/wiki/Equal_temperament) (say, in [microtonal music](https://en.wikipedia.org/wiki/Microtonal_music)) and there can be not enough examples for some of them. A model cannot be trained in a supervised manner without a dataset, but, given some rules, an agent can be trained and evaluated even for an absolutely new tuning system.\n\nTo find more details, look at [a draft of a paper](https://github.com/Nikolay-Lysenko/rl-musician/blob/master/docs/paper/paper.pdf).\n\n## Installation\n\nTo install a stable version, run:\n```bash\npip install rl-musician\n```\n\n## Usage\n\nTo train an agent from scratch and to get results produced by it, run:\n```bash\npython -m rlmusician \\\n    -c [path_to_your_config] \\\n    -p [how_long_to_train] \\\n    -e [how_many_pieces_to_produce] \\\n    -w [path_to_a_file_with_startup_weights]\n```\n\nAll four arguments are optional. [Default config](https://github.com/Nikolay-Lysenko/rl-musician/blob/master/rlmusician/configs/default_config.yml) is used if `-c` argument is not passed; `-p` and `-e` options have reasonable defaults too. You should skip `-w` option if you have no saved weights from previous launches. If `-w` option is omitted, startup weights are drawn at random with zero mean.  \n\nTraining with default settings takes about 30 minutes on a CPU of a regular laptop. If you are on Mac OS, please check that [parallelism is enabled](https://stackoverflow.com/questions/50168647/multiprocessing-causes-python-to-crash-and-gives-an-error-may-have-been-in-progr).\n\nCreated pieces are stored in a directory specified in the config. For each piece, there is a nested directory that contains:\n* Piano roll in TSV format;\n* MIDI file;\n* Events file in [sinethesizer](https://github.com/Nikolay-Lysenko/sinethesizer) TSV format;\n* WAV file.\n\nAlso, weights of the trained agent are saved in a timestamped file.\n", 'contents': "['.codeclimate.yml', '.coveragerc', '.gitignore', '.travis.yml', 'LICENSE', 'README.md', 'docs', 'requirements.txt', 'requirements', 'rlmusician', 'setup.cfg', 'setup.py', 'tests']", 'stars': 19, 'watchers': 19, 'forks': 4, 'deprepos': '0', 'deppacks': '0'}