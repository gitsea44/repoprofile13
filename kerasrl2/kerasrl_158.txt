{'dependent': 'kerasrl', 'repo': '/Sardhendu/DeepRL', 'language': 'Jupyter Notebook', 'description': 'Deep Reinforcement Learning Projects', 'readme': '\n\n# Implemented Algorithms:\n\n#### [Deep-Q-Network (DQN) and Double-DQN](https://github.com/Sardhendu/DeepRL/blob/master/src/navigation/agent.py)   \n\n#### [REINFORCE - Policy gradient](https://github.com/Sardhendu/DeepRL/blob/master/src/pong_atari/agent.py)\n\n#### [Deep Deterministic Policy Gradient (DDPG)](https://github.com/Sardhendu/DeepRL/blob/master/src/continuous_control/agent.py)\n\n#### [Multi Agent Deep Deterministic Policy Gradient (MADDPG)](https://github.com/Sardhendu/DeepRL/blob/master/src/collab_compete/agent.py) \n    \n\n[//]: # (Image References)\n\n[image1]: https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif "Trained Agent"\n\nProject 1: Navigation [*Link*](https://github.com/Sardhendu/DeepRL/tree/master/src/navigation)\n-----------\n\nTrain an agent to navigate (and collect bananas!) in a large, square world.  \n\n![Trained Agent][image1]\n     \n   * **Task:** Episodic\n   * **Reward:** +1 for collecting a yellow banana, -1 is provided for collecting a blue banana.  \n   * **State space:** \n      * **Vector Environment:** 37 dimensions that includes agent\'s velocity, along with ray-based perception of objects around agent\'s forward direction.  \n      * **Visual Environment:** (84, 84, 3) where 84x84 is the image size.\n   * **Action space:** \n       - **`0`** - move forward.\n       - **`1`** - move backward.\n       - **`2`** - turn left.\n       - **`3`** - turn right.\n         \n[//]: # (Image References)\n\n[image2]: https://user-images.githubusercontent.com/10624937/43851024-320ba930-9aff-11e8-8493-ee547c6af349.gif "Trained Agent"\n\n\n Project 2: Continuous Control [*Link*](https://github.com/Sardhendu/DeepRL/tree/master/src/continuous_control)\n-----------\n\nTrain an agent (double-jointed arm) is to maintain its position at the target location for as many time steps as \npossible \n\n![Trained Agent][image2]\n \n   * **Task:** Continuous\n   * **Reward:** +0.1 for agent\'s hand in the goal location  \n   * **State space:** \n      * **Single Agent Environment:** (1, 33)\n         * 33 dimensions consisting of position, rotation, velocity, and angular velocities of the arm.\n         * 1 agent   \n      * **Multi Agent Environment:** (20, 33)\n         * 33 dimensions consisting of position, rotation, velocity, and angular velocities of the arm.\n         * 20 agent \n   * **Action space:** \n       Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.\n \n \n[//]: # (Image References)      \n       \n[image3]: https://user-images.githubusercontent.com/10624937/43851646-d899bf20-9b00-11e8-858c-29b5c2c94ccc.png "Crawler"\n\n![Trained Agent][image3] \n            \n \n[//]: # (Image References)\n\n[image4]: https://user-images.githubusercontent.com/10624937/42135623-e770e354-7d12-11e8-998d-29fc74429ca2.gif "Trained Agent"\n\n\n Project 3: Collaboration and Competition [*Link*](https://github.com/Sardhendu/DeepRL/tree/master/src/collab_compete)\n-----------\n\nTrain two agents to play ping pong. And, the goal of each agent is to keep the ball in play. \n\n![Trained Agent][image4]\n \n   * **Task:** Continuous\n   * **Reward:** +0.1 for hitting the ball over net \n                  -0.01 if the ball hits the ground or goes out of bounds \n   * **State space:** \n      * **Multi Agent Environment:** (2, 24)\n         * 24 dimensions consisting of position, rotation, velocity and etc.   \n   * **Action space:** \n       Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n   * **Target Score**\n        The environment is considered solved, when the average (over 100 episodes) of those **scores** is at least +0.5.\n \n \n \n### TODO:\n1. Modular code for each environment.\n2. Dueling Network Architectures with DQN\n3. Lambda return for REINFORCE (n-step bootstrap)\n4. Apply prioritized experience replay to all the environment and compare while maintaining the modularity of the code.\n5. Actor-Critic\n6. Crawler for Continuous Control\n7. Add Tensorflow graphs instead of manual dictionary graphs for all environments.\n8. Continuous control Test phase.\n9. Parallel Environments and how efficient hte weight sharing is\n\n \n\n## To install with Docker\n\nCreate a docker Image:\n\n   ```bash\n      docker build --tag deep_rl .\n   ```\n\nRun the Image, expose Jupyter Notebook at port 8888 and mount the working directory:\n   ```bash\n      docker run -it -p 8888:8888 -v /path/to/your/local/workspace:/workspace/DeepRL --name deep_rl deep_rl\n   ```\n   \n    \nStart Jupyter Notebook:\n   ```bash\n      jupyter notebook --no-browser --allow-root --ip 0.0.0.0\n   ```', 'contents': "['.DS_Store', '.gitignore', '.idea', '.ipynb_checkpoints', 'Dockerfile', 'NOTES.md', 'Pipfile', 'Pipfile.lock', 'README.md', '__init__.py', '__pycache__', 'models', 'requirements.txt', 'src', 'unity-environment.log']", 'stars': 1, 'watchers': 1, 'forks': 1, 'deprepos': 0, 'deppacks': 0}