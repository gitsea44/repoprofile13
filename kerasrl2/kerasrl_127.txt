{'dependent': 'kerasrl', 'repo': '/UNSW-CEEM/energy-market-deep-learning', 'language': 'Python', 'description': 'Experiments in using deep learning to model competition in liberalised electricity markets.', 'readme': "# Electricity Market Deep Learning Framework.\nThe aim of this software is to provide an adversarial reinforcement learning model of a restructured electricity market, specifically Australia's National Electricity Market. It is intended to work as an environment plugin for Elon Musk & Sam Altman's [OpenAi Gym](https://gym.openai.com/). \n\n## Usage\nKeras, Tensorflow and Python 3.* are required to run the simulations. \nThe software consists of three components: The market simulation webserver, the keras machine reinforcement learning setup, and the user interface. These components communicate in real-time over websockets. \n\n## Demo\nA video of the working UI and simulator can be found [here](https://youtu.be/-A0k6z4WAUY)\nA demo, which works based on whether or not simulations are currently running, can be found [here](https://nem-control.herokuapp.com/)\n\n![screenshot 1](https://user-images.githubusercontent.com/7201209/51462251-7917fa00-1da3-11e9-80c3-294284d20edb.png)\n\n![screenshot 2](https://user-images.githubusercontent.com/7201209/51462319-a4024e00-1da3-11e9-95f4-abe95922cdc2.png)\n\n## Notes on machine learning outcomes\nStart with generators at >0 capacity - otherwise the many variables they have to optimise don't appear consistent with sending generation and price positive - it appears that by reducing generation at a negative price, score is improved.\nTakes about 20,000 runs to get it to learn the market cap. Sometimes a few more\n\nAtari games take ~18 million gens to learn - 30 fps. We do about 50 fps effective, with our sim. So OK speed with websockets.\n\nIncreasing fuzz (ie. epsilon) on the DDPG optimizer seems to instantly help - at least we follow a trend of the demand profile which is interesting.\nI also increased default learning rate by a factor of 10 (0.001 to 0.01)\n\nOne problem is there is barely any discovery with the default settings. I increased sigma in the Ornstein-Uhlenbeck process which has made for a significantly more random appearance in the bidding - but I haven't yet had the model converge (not many runs though). \n\nExtremely good explanation: https://keon.io/deep-q-learning/\n\n\n## References\n\nAt the moment, based on examples and code from Matthias Plappert, repo found here: https://github.com/matthiasplappert/keras-rl\n\n\n## Notes\nMake demand discrete in a tiny range\nThen show each participant their opponents moves last time demand was at that level. \n\nWhat if we kept demand constant? This would really show the equilibrium. Next.\nNo it wouldn't. They would get stuck in wack scenarios for each one max bidding and no competition.\n", 'contents': "['.DS_Store', '.floydexpt', '.floydignore', '.gitignore', '.vscode', '__pycache__', 'adversarial.sh', 'aemo.sh', 'aemo_config.py', 'client.log', 'data', 'dqn_adversarial.py', 'dqn_aemo.py', 'dqn_cartpole.py', 'dqn_simplemarket.py', 'floyd-run.sh', 'floyd.yml', 'floyd_example.sh', 'floyd_requirements.txt', 'local.env.sh', 'logbook_tester.py', 'market_config.py', 'marketsim', 'permutations.py', 'pickles', 'poetry.lock', 'post_from_logbook_file.py', 'pyproject.toml', 'readme.md', 'result_.json', 'result_GEN_1.json', 'result_GEN_2.json', 'run_market_server.py', 'server.log', 'simple.sh', 'space_play.py', 'test_saved_weights.py']", 'stars': 8, 'watchers': 8, 'forks': 2, 'deprepos': 'zero', 'deppacks': 'zero'}