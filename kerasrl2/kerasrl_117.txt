{'dependent': 'kerasrl', 'repo': '/ascvorcov/z80-ddqn', 'language': 'Python', 'description': 'Implementation of Double Dueling Q-Network with prioritized experience replay learning to play zx-spectrum games', 'readme': '# z80-ddqn\nImplementation of Double Dueling Q-Network with prioritized experience replay learning to play zx-spectrum games\n\n## Overview\n\nImplementation of Double Dueling Q-Network with prioritized experience replay learning to play zx-spectrum games.\nUsing keras with plaidml backend, training was done on Radeon RX Vega M GH / Intel Core i7-8809G CPU @ 3.10GHz / 16.0 Gb RAM.\n\nBased on https://github.com/gsurma/atari\n\nGames are run using z80 emulator (https://github.com/ascvorcov/z80emu), every rendered frame is 20msec of real game time.\nNo sound information provided, only screen capture in 16 indexed colors.\n\nFrame skipping can be configured depending on environment. Every 4 frames are converted to grayscale, resized and \ncropped to 84x84 and combined into one "state image", where each "channel" represents a time slice (see sample images below).\n\nModel evaluates state image, suggests action, observes resulting reward and state, adjusts neural network weights\nand repeats this loop until the end of the episode. End-of-episode condition is configured depending on environment.\n\nConvolution neural network architecture:\n![Architecture](./assets/model2.png "Model architecture")\n\n*Loss* - loss function, determined from the difference between true and predicted q-value.\nHuber loss in our case - https://en.wikipedia.org/wiki/Huber_loss\nEach point is an average for 1000 episodes. Lower is better.\n\n*Accuracy* - categorical_accuracy in terms of Keras, \nwhich is \'how often predictions have maximum in the same spot as true values\'. Each point is an average for 1000 episodes.\nhttps://github.com/sagr4019/ResearchProject/wiki/General-Terminology#difference-between-accuracy-and-categorical_accuracy\nHigher is better (falls in range 0..100%)\n\n*Q* - maximum expected q-value by model over the course of one episode. \nEach point is an average for 1000 episodes. Higher is better.\n\n*Score* - max score reached by training model over time. \nEach score point is an average for 10 episodes. Higher is better.\n\n*Step* - max simulation step reached by training model over time. \nEach point is an average for 10 episodes. Higher is better.\n\n## Renegade\n\nModel was training with condition "single knockdown = defeat", to speed up learning rate, with frame skip set to 2.\nAfter 1.5M iterations quality drops due to catastrophic forgetting.\nMaximum reached is 19700 points / 870 simulation steps.\n\nModel did not discover \'throw off a cliff\' strategy on adjacent screen, \nprobably because it is difficult to execute or because it is cheap in terms of points.\nThis strategy is more rewarding for human player since the goal is to advance to next level,\nnot achieve high score. Also model did not discover any of finishing moves.\nInstead, model found a hack to earn infinite amount of points (not actually infinite since episode\nis time-limited) - stunning \'boss\' with a jump-kick does not kill him, earns 100 points, and gives time\nfor another jump-kick. Rarely random move is selected, and boss gets killed.\nModel is completely lost on next level, since both background color and enemy type is different.\n\nHuman and model results below are for single life loss.\n\nSubj|Games|Max|Avg|Mean|StdDev\n--|--|--|--|--|--|\nrandom|100|3850|872|550|775.1611107\nmodel|100|19700|5939.5|4750|4097.999604\nhuman|10|19600|6145|4725|4746.604049\n\n<table>\n<tr>\n<td><img src="/assets/renegade_trained_frame_min.gif"  width="352" height="312"/></td>\n<td><img src="/assets/renegade_trained_state_min.gif" width="352" height="312"/></td>\n</tr>\n<tr>\n<td><img src="/assets/renegade_accuracy.png"/></td>\n<td><img src="/assets/renegade_loss.png"/></td>\n<td><img src="/assets/renegade_q.png"/></td>\n</tr>\n<tr>\n<td><img src="/assets/renegade_score.png"/></td>\n<td><img src="/assets/renegade_step.png"/></td>\n</tr>\n</table>\n\nActivation heatmap for first convolutional layer of DDQN:\n[![Activation heatmap for first convolutional layer of DDQN](http://img.youtube.com/vi/gLpF19FDtSM/0.jpg)](http://www.youtube.com/watch?v=gLpF19FDtSM)\n\n## Xecutor\n\nModel was training with condition "loss of one life = end of episode".\nAfter 45K episodes frame skipping was changed from 2 to 1, which can be seen on a chart.\nMaximum score reached is 3500 / 160 simulation steps.\n\nModel doesn\'t "see" enemy bullets, and overall behavior looks random.\nBonus pickup and weapon change also look accidental.\nDifficult game for machine learning, I could not get acceptable results by changing hyperparameters.\nPerhaps the problem is with constantly changing background, larger delay beween the shot and reward,\nand background noise of moving "stars".\n\nBoth human and model results below are for full game with 3 lives.\n\nSubj|Games|Max|Avg|Mean|StdDev\n--|--|--|--|--|--|\nrandom|100|1700|602.5|600|321.4059235\nmodel|100|3250|2075|2125|427.9089396\nhuman|10|2050|1470|1575|481.433046\n\n<table>\n<tr>\n<td><img src="/assets/xecutor_trained_frame_min.gif"  width="352" height="312"/></td>\n<td><img src="/assets/xecutor_trained_state_min.gif" width="352" height="312"/></td>\n</tr>\n<tr>\n<td><img src="/assets/xecutor_accuracy.png"/></td>\n<td><img src="/assets/xecutor_loss.png"/></td>\n<td><img src="/assets/xecutor_q.png"/></td>\n</tr>\n<tr>\n<td><img src="/assets/xecutor_score.png"/></td>\n<td><img src="/assets/xecutor_step.png"/></td>\n</tr>\n</table>\n\nActivation heatmap for first convolutional layer of DDQN:\n[![Activation heatmap for first convolutional layer of DDQN](http://img.youtube.com/vi/1iGw8KeiQwM/0.jpg)](http://www.youtube.com/watch?v=1iGw8KeiQwM)\n\n## Barbarian\n\nModel was training with condition "enemy scored > FF points = end of episode", to speed up learning rate.\nFrame skipping set to 2, entire game field does not fit into 84 pixels, only left part is captured \n(where most of the action takes place). I could not find where \'hit points\' are stored in memory, so \nthe only way to train was to focus on enemy score. Initial condition was \'enemy scored > 0 = end of episode",\nbut this proved to be a bad strategy since the model could not learn how to recover after being hit.\n\nModel learned a few nice combo moves (see gif below - fencing followed by upper strike when cornered),\ndoesn\'t attack and prefer to defend in the corner.\nUnfortunately (as always) completely lost on a second level, probably due to a different background color.\n\nAt the beginning of learning process model discovered "beheading" fatality move, but abandoned it since\nit is difficult to execute, although it pays a significant reward of 500 points. With normal sparring,\nsingle level can earn 1300-1400 points.\n\nSubj|Games|Max|Avg|Mean|StdDev\n--|--|--|--|--|--|\nrandom|100|1700|340|200|351.1884584\nmodel|100|2700|1546.5|1550|311.4202559\nhuman|10|5150|1590|1375|1402.735423\n\n<table>\n<tr>\n<td><img src="/assets/barbarian_trained_frame_min.gif"  width="352" height="312"/></td>\n<td><img src="/assets/barbarian_trained_state_min.gif" width="352" height="312"/></td>\n</tr>\n<tr>\n<td><img src="/assets/barbarian_accuracy.png"/></td>\n<td><img src="/assets/barbarian_loss.png"/></td>\n<td><img src="/assets/barbarian_q.png"/></td>\n</tr>\n<tr>\n<td><img src="/assets/barbarian_score.png"/></td>\n<td><img src="/assets/barbarian_step.png"/></td>\n</tr>\n</table>\n\nActivation heatmap for first convolutional layer of DDQN:\n[![Activation heatmap for first convolutional layer of DDQN](http://img.youtube.com/vi/Py-aSZYm3Qs/0.jpg)](http://www.youtube.com/watch?v=Py-aSZYm3Qs)\n\n## River raid\n\nModel was training with condition "loss of one life = end of episode", with frame skipping set to 2.\nFor quite a long time model could not learn the initial moves and died at the beginning of episode,\nwhich can be seen on the chart - average score is close to 100. Ultimately, final score is limited by\namount of "fuel", since model did not learn "refueling" move - it is not giving any points, and delayed reward\nis too far in the future. Maximum reached is 1840 points / 156 steps. \n\nHuman and model results below are for single life loss.\n\nSubj|Games|Max|Avg|Mean|StdDev\n--|--|--|--|--|--|\nrandom|100|930|217.5|170|158.077944\nmodel|100|1840|1501|1660|436.6180866\nhuman|10|14150|6351|5285|4598.628902\n\n<table>\n<tr>\n<td><img src="/assets/riverraid_trained_frame_min.gif"  width="352" height="312"/></td>\n<td><img src="/assets/riverraid_trained_state_min.gif" width="352" height="312"/></td>\n</tr>\n<tr>\n<td><img src="/assets/riverraid_accuracy.png"/></td>\n<td><img src="/assets/riverraid_loss.png"/></td>\n<td><img src="/assets/riverraid_q.png"/></td>\n</tr>\n<tr>\n<td><img src="/assets/riverraid_score.png"/></td>\n<td><img src="/assets/riverraid_step.png"/></td>\n</tr>\n</table>\n\nActivation heatmap for first convolutional layer of DDQN:\n[![Activation heatmap for first convolutional layer of DDQN](http://img.youtube.com/vi/MffK-BjuVpo/0.jpg)](http://www.youtube.com/watch?v=MffK-BjuVpo)\n\n## Krakout\n\nAs usual, training with condition "loss of one life = end of episode", 2 frame skipping.\nOtherwise model is not punished for life loss and training is slowed down / quality drops.\nAfter 35K games model learns to track and deflect ball, but then the goal of game is\nreplaced. Model discovers that hitting \'monsters\' yields 100 points, while hitting blocks\nyields only 10-20 points, so the goal to hit high score is to hit as many monsters as possible,\ninstead of going to next level. At the peak model can score up to 8K points on first level,\nwhile there are only 44 blocks, yielding no more than 1000 points total.\n\nBoth human and model results below are for full game with 3 lives.\n\nSubj|Games|Max|Avg|Mean|StdDev\n--|--|--|--|--|--|\nrandom|100|1790|118.1|40|300.4699854\nmodel|100|9560|1941.9|1545|1809.13099\nhuman|10|14880|5785|4060|4299.227967\n\n<table>\n<tr>\n<td><img src="/assets/krakout_trained_frame_min.gif"  width="352" height="312"/></td>\n<td><img src="/assets/krakout_trained_state_min.gif" width="352" height="312"/></td>\n</tr>\n<tr>\n<td><img src="/assets/krakout_accuracy.png"/></td>\n<td><img src="/assets/krakout_loss.png"/></td>\n<td><img src="/assets/krakout_q.png"/></td>\n</tr>\n<tr>\n<td><img src="/assets/krakout_score.png"/></td>\n<td><img src="/assets/krakout_step.png"/></td>\n</tr>\n</table>\n\nActivation heatmap for first convolutional layer of DDQN:\n[![Activation heatmap for first convolutional layer of DDQN](http://img.youtube.com/vi/5nSFqYxiEvA/0.jpg)](http://www.youtube.com/watch?v=5nSFqYxiEvA)\n', 'contents': "['.gitignore', 'LICENSE', 'README.md', 'assets', 'base_game_model.py', 'convolutional_neural_network.py', 'ddqn_game_model.py', 'emulator.py', 'env_barbarian.py', 'env_default.py', 'env_krakout.py', 'env_raiders.py', 'env_renegade.py', 'env_riverraid.py', 'env_xecutor.py', 'frame.py', 'gym_wrappers.py', 'heatmap.py', 'image_viewer.py', 'logger.py', 'main.py', 'memory.py', 'models', 'renderer.py', 'requirements.txt', 'roms', 'speccy.py', 'speccybw.py', 'speccyfilt.py', 'sumtree.py', 'view.html', 'z80native.dll', 'z80wrapper.py']", 'stars': 0, 'watchers': 0, 'forks': 0, 'deprepos': 0, 'deppacks': 0}