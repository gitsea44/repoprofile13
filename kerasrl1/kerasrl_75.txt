{'dependent': 'kerasrl', 'repo': '/chairbender/fantasy-football-auction-ai', 'language': 'Python', 'description': 'An AI for doing a fantasy football auction', 'readme': '# fantasy-football-auction-ai\nNOTE: This project is indefinitely on hold for the forseeable future. Stuff I learned:\n* Deep RL is not a super mature field - there are libraries that I expect would exist right now\nbut which don\'t actually exist!\n* Got some experience building and debugging an RL agent / network architecture\n* Got experience building an RL environment (the fantasy football api) / working with python\n* It\'s quite hard to figure out why something is or isn\'t learning\n* Deep RL is not magic (yet), and actually might not be the best approach for solving certain problems. See\n (Deep RL Doesn\'t Work Yet)[https://www.alexirpan.com/2018/02/14/rl-hard.html]\n* It\'s hard to even get a reliable behavior on the SAME architecture / hyperparameters / environment,\nrunning multiple times. Randomness plays quite a big part.\n* Overall, there\'s lots of hype for Deep RL but it\'s not quite there yet.\n* I suspect that my toy problem may be better solved by other methods, if I actually cared about \nsolving it rather than just using it as a way to learn this stuff.\n* It\'s quite hard to find best practices / principles / well-worn paths / "giants shoulders" within\nDeep RL. The people who know how to do it well seem to mostly have that info in their heads. \nFor Deep RL, we don\'t seem to have good resources on design and practice unless you spend a lot of\ntime keeping up on the research and have the knowledge to be able to synthesize that info. This\nisn\'t the sort of thing where a smart programmer can just read a blog post, grab a library off the\nshelf, implement some adapters, and let the library do the rest. It\'s not even comparable to ML -\nthere\'s actually quite a lot of info on ML in general, concerning design and practice.\n* So, I\'m updating my expectations for applying Deep RL to problems. It\'s not as simple as you\nmight think, coming from a "Deep RL is magic" perspective. You see the successes and not the failures.\nAnd you don\'t see how much time goes into things you wouldn\'t think about.\n* Just as an example, here\'s some things that I didn\'t expect to spend so much time on: figuring out what\nis going on inside my model, figuring out if my environment observations have sufficient info for learning,\nfiguring out if there\'s a bug in my environment observations. I think just trying to figure out\nand debug why an agent isn\'t succeeding is probably the majority of the time one would spend on\na Deep RL project. It\'s hard to figure out what deep nets are doing. If there is a good way to do this,\nI didn\'t find one.\n* Maybe in 5 years, this will be much easier.\n\nAn agent for doing a fantasy football auction, using the gym fantasy football auction environment\n\nSome code was based on https://github.com/Zeta36/chess-alpha-zero to implement the "alpha zero"\nlearning agent.\n\nTo run:\n````\npip install .\npython learn.py\n````\n\n# Performance\n\nThis shows how long it takes to solve each task.\n\n| Task  | Agent | Episodes |\n| ------------------------------------------------------------------- | ------------------------------ | ----- |\n| FFEnv1-v0 (reward 3)  | ConvDQNFantasyFootballAgent | 469   |\n\n', 'contents': "['.gitmodules', 'LICENSE.txt', 'NOTES.txt', 'README.md', 'fantasy_football_auction_ai', 'learn.py', 'setup.py']", 'stars': 2, 'watchers': 2, 'forks': 1, 'deprepos': 0, 'deppacks': 0}