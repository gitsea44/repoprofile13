{'dependent': 'kerasrl', 'repo': '/CSCI4850/S18-team3-project', 'language': 'TeX', 'description': None, 'readme': "# Team Pandemonium\n\n## Demo\n\nOur demo file - `generate_text_demo.ipynb` - gives a quick and simple example of our results. This code very clearly shows that including the part of speech tag as input to the network, along with the word itself, gives qualitatively better results than just feeding in the word as input. Running each code block in the iPython notebook will allow you to try both ways with the exact same model. \n1. Install Jupyter: http://jupyter.readthedocs.io/en/latest/install.html\n\n2. Open `generate_text_demo.ipynb`\n\nAs mentioned above, running this code will give you an idea of the text that can be generated with our model, both without and with parts of speech appended to the input. While the sentences generated from the network with parts of speech appended are visibly better, the sentences still are not great. Below is a detailed walkthrough to help you manipulate the network and possibly get better results! Investigate deeper, larger networks, tweak the hyperparameters of the model, and consider other embedding techniques.\n\n\n## Quick Start Guide \n\nIf you have all the dependences (outlined below), run the following scripts in order from the root directory:\n\n1. ```python utils/preprocess.py```\n\n2. ```python train.py --include_grammar y```\n\n3. ```python test.py --include_grammar y```\n\n\nThis will download and preprocess the data, train an RNN with parts of speech included, and generate 50 sentences based on the trained model.\n\n## Detailed Walkthrough\n\n<details>\n<summary>Installing Dependencies</summary>\n<br>\n\n### Installing Dependences\nInstall the python package manager PIP  \n\n__NOTE:__ Make sure to use python 3.\n\n```python get-pip.py``` or ```sudo easy_install pip```  \n\n### Install virutalenv\ninstall virtualenv using pip:  \n\n```pip install virtualenv```\n\nvirtualenv is used to create and manage environments for different python projects.  Use virtualenv to create a virtual environment by using:\n\n```virtualenv env```\n\nto create a folder named env which will store relevant python related files.  \n\n__NOTE:__ If you are in a virtual environment -- do not use ```sudo```, as this will not install into the virtual environment, but the system's environment instead.\n\n### Switching environments\nUse ```source env/bin/activate``` to load your environment.  ```env``` is a placeholder for your environment name.  \n\nuse ```deactivate``` to exit out of the virtual environment.  \n\nTo verify which environment you're in, use ```which pip```.  if you see that the pip location is in your environment folder (env), then you are in your virtual environment.  Also notice that in virtualenv, python3 is now the default interpreter which makes life much easier.  Check using ```python --version``` and notice the __lack__ of 3 at the end.\n\n### Install packages\nInstalling python packages can be done by using the `requirements.txt`:  \n```pip install -r requirements.txt```\n</details>\n\n<details>\n<summary>Data Preprocessing</summary>\n<br>\n\n## Data preprocessing and preparation\n\n### NLTK parts of speech data  \n\nThe NLTK library requires you to download corpora data for our part of speech tagging.  You can download and install this data via a python interpreter:\n\n```\n>>> import nltk  \n>>> nltk.download()\n```\n\nA dialog window will pop-up after the function call which allows you to select and install data.\n\n## Downloading and preprocessing transcript data  \nOur project uses the American cartoon show, Rick and Morty, to create our training corpus. The script can be acquired by running ```python utils/preprocess.py``` from the root directory, which will download, preprocess, and format the data, placing it in `/data/train/cleaned/simple.txt`\n\n</details>\n\n<details>\n<summary>Training and Generating</summary>\n<br>\n\n## Training and Testing \n\n### Train\n`train.py` is where the model is located, and this is where you can tweak any hyperparameters you'd like.  Then run\n\n```python train.py --include_grammar y``` or\n\n```python train.py --include_grammar n```,\n\nwhich will train the model on any data that is located in `data/train/cleaned/simple.txt`.\n\nDepending on the provided argument, training will occur with or without each word's corresponding part of speech, and\nalso produces an image of the training curves, called `training_curves_pos.png` or `training_curves_no_pos.png`.\n\n### Generate Text \nAfter the network is trained, you can run \n\n```python test.py --include_grammar y``` or\n\n```python test.py --include_grammar n```,\n\nwhich will load the model and generate 50 sentences.\nSimilar to above, the output will be stored in `pos_output.txt` or `no_pos_output.txt`, depending on the provided arguments.\n</details>\n\n<details>\n<summary>Comparison Metrics</summary>\n<br>\n\n## Comparisons/Metrics\n\nNow that you have generated some sentences, its time to do some comparisons to get some statistics.\n### Statistics\nRunning ```python stats.py``` will calculate the total number of words in your corpus, the total number of unique words in your corpus, and then the outlier count (words that appear 5 times or less). These statistics will be written to stat.txt\n\n### Markov Sentences\nRunning ```python markov.py``` will generate 50 markov sentences based on the text stored in `data/train/cleaned/simple.txt`.\nThe markov sentences are written to a file named `markovSentences.txt`.\n\n### Metrics\nmetrics.py is a script that takes two .txt files as arguments and compares each file sentence by sentence, calculating the average hamming, cosine, Gotoh and Levenshtein distances between the sentences. It compares the part of speech tags of the words in each sentence, as opposed to the actual words. \n\nUse ```python metrics.py --file1 firstFilename.txt --file2 secondFilename.txt``` to compare the distances between the two files. It will output the distances as it runs, and then writes the statistics to `metricStats.txt`.\nTo compare the sentences generated by the markov model and the sentences generated by the neural network, run\n\n```python metrics.py --file1 markovSentences.txt --file2 ../pos_output.txt```\n\n</details>\n", 'contents': "['.gitignore', 'LICENSE', 'README.md', 'course_materials', 'generate_text_demo.ipynb', 'models', 'requirements.txt', 'results', 'test.py', 'train.py', 'utils']", 'stars': 0, 'watchers': 0, 'forks': 0, 'deprepos': 0, 'deppacks': 0}