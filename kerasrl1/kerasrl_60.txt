{'dependent': 'kerasrl', 'repo': '/calvinloveland/MancalaAI', 'language': 'Python', 'description': None, 'readme': "# MancalaAI\n\nA program for training and testing networks to play Mancala\n\n## Running MancalaAI\n\nDependencies:\n\n\tPython 3.5 or greater\n\tOpenAI Gym\n\tKeras-RL\n\tTheano\n\nThe easiest way to get these dependencies is to simply run:\n\n```\npip install -r requirements.txt\n```\n\nThen edit $HOME/.keras/keras.json to use theano as a backend\n\nI've tested MancalaAI on Windows and Ubuntu 16.04 on Ubuntu I also had to install python3-dev\n\n```\nsudo apt install python3-dev\n```\n\nOnce you have the dependencies you can run unit tests or main:\n\n```\npython3 __main__.py\npython3 unit_tests.py\n```\n\n## Deliverables\n\nI did train networks to play Mancala but they are really really\nbad at it. You can play Mancala against any network you chose.\nAdditionally the OpenAI Gym I've created can be used with any\nother reinforcement learning system that supports Gym. More \ninformation below.\n\n### Networks\nI've technically delivered everything promised here.\nI've trained lots of networks to play Mancala. The only problem\nis that every single network is absolutely terrible. For some reason\nnot a single network can avoid making invalid moves let alone choose the moves\nof a decent Mancala player. You can see this when playing against\nthe network. You may be able to get a few moves in but very quickly\nthe network will make an invalid move and lose. I'm not sure whe I was\nunable to get a network to train properly. It was not for a lack of trying.\nI tried a number of different network architectures (You can see the models\nwhen testing the networks). I also tried many different \noptimizers, reward functions, learning rates, decay rates, and action policies.\nOverall I would estimate my i7-4770 CPU spent 72 hours solid training \n 5 networks at a time\nat 100% utilization. I trained most networks for between 10,000 and\n100,000 steps. \n\n### Play Against MancalaAI\nYou can currently play against the best network but it will not \nbe a very hard game. The game against a user is a Gym just like\nthe training board. This means it would be possible to train\na network against user input. However I really just did it this\nway because it was easier.\n\n### Mancala Gym\nFor the Mancala Gym I only implement the random input and user input\nversion. I was planning on creating one using a MinMax algorithm\nbut since the networks could not beat\nthe Gym making random moves I didn't see the point of creating\na harder challenge. \n\n## Resources\n\nhttps://github.com/plaidml/plaidml PlaidML - The original backend I tried\nto use in the hopes I could train networks using my GPU. I couldn't\nget it working with Keras-RL\n\nhttps://gym.openai.com OpenAI Gym - Provides an easy way\nto create a reinforcement learning environment.\n\nhttps://github.com/keras-rl/keras-rl Keras-RL - An old research\nproject that is supposed to provide an easy reinforcement \nlearning library. This hasn't had much support in about a year\nand the documentation is very sparse. If I had to start over I'd \nprobably choose a different library\n\nhttps://github.com/openai/gym-soccer Soccer Gym - An easy to follow\nexample of creating a custom Gym. Covers a lot of information\nthat the Gym documentation misses.\n", 'contents': "['.gitignore', 'History.txt', 'InvalidMove.PNG', 'README.md', '__main__.py', 'agent.py', 'gym_mancala', 'model.py', 'networks', 'requirements.txt', 'shared', 'unit_tests.py']", 'stars': 0, 'watchers': 0, 'forks': 0, 'deprepos': 0, 'deppacks': 0}