{'dependent': 'kerasrl', 'repo': '/RsbCode/gym-malware', 'language': 'Python', 'description': None, 'readme': '# Malware Env for OpenAI Gym\r\n**************************\r\n\r\n**This is a malware manipulation environment for OpenAI\'s ``gym``.** \r\n[OpenAI Gym](https://gym.openai.com/) is a toolkit for developing and comparing reinforcement \r\nlearning algorithms. This makes it possible to write agents that learn \r\nto manipulate PE files (e.g., malware) to achieve some objective \r\n(e.g., bypass AV) based on a reward provided by taking specific manipulation\r\nactions.\r\n\r\nObjective\r\n======\r\nCreate an AI that learns through reinforcement learning which functionality-preserving transformations to make on a malware sample to break through / bypass machine learning static-analysis malware detection.\r\n\r\n![Breakout](https://github.com/matthiasplappert/keras-rl/raw/master/assets/breakout.gif?raw=true\r\n"Breakout")\r\n\r\nBasics\r\n======\r\n\r\nThere are two basic concepts in reinforcement learning: the environment (in our case, the malware sample) and the agent (namely, the algorithm used to change the environment). The agent sends `actions` to the environment, and the environment replies with `observations` and `rewards` (that is, a score).\r\n\r\nThis repo provides an environment for manipulating PE files and providing rewards that are based around bypassing AV.  An agent can be deployed that have already been written for the rich ``gym`` framework.  For example\r\n\r\n* https://github.com/pfnet/chainerrl [recommended]\r\n* https://github.com/matthiasplappert/keras-rl\r\n \r\nSetup\r\n=====\r\nThe EvadeRL framework is built on Python3.6 we recommend first creating a virtualenv (details can be found [here]) with Python3.6 then performing the following actions ensure you have the correct python libraries:\r\n\r\n[here]: https://docs.python.org/3/tutorial/venv.html\r\n```sh\r\npip install -r requirements.txt\r\n```\r\n\r\nEvadeRL also leverages a Library to Instrument Executable Formats aptly named [LIEF]. It allows our agent to modify the binary on-the-fly. To add it to your virtualenv just ```pip install``` one of their pre-built packages. Examples below:\r\n\r\n[LIEF]: https://github.com/lief-project/LIEF\r\n\r\nLinux\r\n```\r\npip install https://github.com/lief-project/LIEF/releases/download/0.7.0/linux_lief-0.7.0_py3.6.tar.gz\r\n```\r\n\r\nOSX\r\n```\r\npip install https://github.com/lief-project/LIEF/releases/download/0.7.0/osx_lief-0.7.0_py3.6.tar.gz\r\n```\r\n\r\nOnce completed ensure you\'ve moved malware samples into the \r\n```\r\ngym_malware/gym_malware/envs/utils/samples/\r\n```\r\n\r\nIf you are unsure where to acquire malware samples see the **Data Acquisition** section below. If you have samples in the correct directory you can check to see if your environment is correctly setup by running :\r\n\r\n```\r\npython test_agent_chainer.py\r\n```\r\n\r\nNote that if you are using Anaconda, you may need to\r\n```\r\nconda install libgcc\r\n```\r\nin order for LIEF to operate properly.\r\n\r\nData Acquisition\r\n=====\r\nIf you have a VirusTotal API key, you may download samples to the `gym_malware/gym_malware/envs/utils/samples/` using the Python script `download_samples.py`.\r\n\r\nGym-Malware Environment\r\n====\r\nEvadeRL pits a reinforcement agent against the malware environment consisting of the following components:\r\n\r\n* Action Space\r\n* Independent Malware Classifier\r\n* OpenAI framework malware environment (aka gym-malware)\r\n \r\nAction Space\r\n----\r\nThe moves or actions that can be performed on a malware sample in our environment consist of the following binary manipulations:\r\n* append_zero\r\n* append_random_ascii\r\n* append_random_bytes\r\n* remove_signature\r\n* upx_pack\r\n* upx_unpack\r\n* change_section_names_from_list\r\n* change_section_names_to random\r\n* modify_export\r\n* remove_debug\r\n* break_optional_header_checksum\r\n\r\nThe agent will randomly select these actions in an attempt to bypass the classifier (info on default classifier below). Over time, the agent learns which combinations lead to the highest rewards, or learns a policy (*like an optimal plan of attack for any given observation*).\r\n\r\nIndependent Classifier\r\n----\r\nIncluded as a default model is a [gradient boosted decision trees model] trained on 50k malicious and 50k benign samples with the following features extracted:\r\n* Byte-level data (e.g. histogram and entropy)\r\n* Header\r\n* Section\r\n* Import/Exports\r\n\r\n\r\n[gradient boosted decision trees model]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\r\n\r\n\r\n', 'contents': "['.gitignore', 'LICENSE', 'README.md', 'download_samples.py', 'gym_malware', 'models', 'requirements.txt', 'setup.py', 'test_agent_chainer.py', 'test_agent_kerasrl.py', 'train_agent_chainer.py', 'train_agent_kerasrl.py']", 'stars': 1, 'watchers': 1, 'forks': 1, 'deprepos': 0, 'deppacks': 0}