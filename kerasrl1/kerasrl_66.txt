{'dependent': 'kerasrl', 'repo': '/Kostis-S-Z/trading-rl', 'language': 'Python', 'description': 'Deep Reinforcement Learning for Financial Trading using Price Trailing @ ICASSP 2019', 'readme': '\n## Deep Reinforcement Learning for financial trading using keras-rl\n\nThis code is part of the paper "Deep Reinforcement Learning for Financial Trading using Price Trailing" [[1]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8683161) presented at ICASSP 2019.\n\n### Getting Started\n\nTwo models were developed in this project. \n\nThe first model (Trail Environment) is our proposed method which is explained analytically the paper above. \n\nThe second model (Deng_Environment) was based on the paper "Deep Direct Reinforcement Learning for Financial Signal Representation and Trading" [[2]](http://cslt.riit.tsinghua.edu.cn/mediawiki/images/a/aa/07407387.pdf) and was used to set a baseline to compare our results. For more information on how this model was used, please refer to our paper of ICASSP 2019.\n\nFor both approaches a DQN Agent as implemented in keras-rl [[3]](https://github.com/keras-rl/keras-rl) was used. The environment of the agent was built from scratch to provide a framework to a dataset of Forex exchange rates between two currencies.\n\n### Prerequisites & Installation\n\nI highly suggest using a virtual environment (e.g venv) to run this project.\n\nThe code is based mainly on the following packages:\n- Python 2.7 / 3.5\n- Tensorflow | see installation @ www.tensorflow.org/install/\n- OpenAI Gym | pip install gym\n- Keras + Keras-rl | pip install keras && keras-rl\n- bokeh | pip install bokeh\n- pandas | pip install pandas\n- h5py | pip install h5py\n\nTo easily install all dependencies, run:\n\n```\npip install -r requirements.txt\n```\n\n*Note: if you are using python 2.7 then there is only one line of code you need to change to make it work and can be found within the environments file*\n\nPython 2\n```\nfor var, default in var_defaults.iteritems():\n```\n\nPython 3\n```\nfor var, default in var_defaults.items():\n```\n\n\n### Hardware\n\nThe code can be run on a CPU or a GPU. You can also choose has much RAM and how many CPU cores to allocate when training a model. This allows multiple models to be trained in parallel. You can change the parameters in "dqn_agent.py"\n\n```python\nCPU = True  # Selection between CPU or GPU\nCPU_cores = 2  # If CPU, how many cores\nGPU_mem_use = 0.25  # In both cases the GPU mem is going to be used, choose fraction to use\n\n```\n\nEach epoch usually takes about ~7(GTX 750 ti)/14(i7-7700) seconds (depending on different variable settings). The models were trained on GTX 750ti 2GB GRAM, which usually would take anything from 30min to 2.5hr.\n\n### Dataset\n\nThis project was trained and tested with data of US-Euro exchange rates collected over many years. Feel free to try different currencies. The form of the data should be in a usual time-series format as a numpy 1-D array with shape e.g (1000, ).\n\n**Note:** A lot of data of exchange rates found online and are free, can sometimes be incomplete or in a bad format. Usually some preprocessing needs to be done in order to get good results.\n\n\n### Execute\n\nThe structure should look likes this\n\n    .\n    ├── trading_agent           # Folder that contains an agent, an environment and code to calculate the PnL\n    │   ├── dqn_agent.py            # Code of the agent to execute\n    │   └── Environments            # The environments of the agent\n    |\n    ├── data                  # Folder that contains a train and a test dataset\n    │   ├── train_data.npy          # Train data of exchange rates\n    │   └── test_data.npy           # A different dataset containing exchange rates to test on\n    └── ...\n\nThe file _Environment_ is a parent class that contains methods that are used by both environments or that should be implemented. The files *TrailEnv* and *DengEnv* extend this class and implement environment-specific methods. Currently, a method To choose between environments use this variable:\n\n```\nMETHOD = trailing  # trailing or deng\n```\n\nThen simply move to the directory of the agent you want to run and execute:\n\n```\npython dqn_agent.py\n```\n\nThanks to keras, you can also stop training whenever you like and the model will save its progress and weights!\n\n### Results\n\nFor each model you create a new directory will be created containing many useful information, such as:\n- ***agent_info.json***: The parameter values of the model\n- ***model.json***: The model structure which you can load back to keras in the future\n- ***memory.npy***: The history of the states of the agent\n- ***weights_epoch_X.h5f***: The weights of the model in the best epoch\n- Information regarding training e.g rewards, the trades the agent made, pnl\n- A folder for each test dataset it was tested on\n\nAdditionally, a history of the agents trades will show up in your browser using the bokeh environment.\n\n### Note\n\n_Unfortunately, this project is no longer active and I haven\'t had the chance to work on it for some time now, but I really appreciate that people are interested and I will try my best to answer any questions or fix bugs through pull requests. Note, that my answers might be slow at times since I need a bit of time to re-familiarize myself with the project before moving on to debug._\n\n### References\n\n1. K. S. Zarkias, N. Passalis, A. Tsantekidis and A. Tefas, "Deep Reinforcement Learning for Financial Trading Using Price Trailing," ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, United Kingdom, 2019, pp. 3067-3071.\ndoi: 10.1109/ICASSP.2019.8683161 URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8683161&isnumber=8682151\n\n2. Y. Deng, F. Bao, Y. Kong, Z. Ren and Q. Dai, "Deep Direct Reinforcement Learning for Financial Signal Representation and Trading," in IEEE Transactions on Neural Networks and Learning Systems, vol. 28, no. 3, pp. 653-664, March 2017.\ndoi: 10.1109/TNNLS.2016.2522401\n\n3. *Keras-RL* [github](https://github.com/keras-rl/keras-rl)\n\n\n### Citation\n\n```\n@INPROCEEDINGS{8683161,\nauthor={K. S. {Zarkias} and N. {Passalis} and A. {Tsantekidis} and A. {Tefas}},\nbooktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\ntitle={Deep Reinforcement Learning for Financial Trading Using Price Trailing},\nyear={2019},\nvolume={},\nnumber={},\npages={3067-3071},\nkeywords={Deep Reinforcement Learning;Financial Markets;Price Forecasting;Trading},\ndoi={10.1109/ICASSP.2019.8683161},\nISSN={2379-190X},\nmonth={May},}\n```\n\n', 'contents': "['Trailing.pdf', 'README.md', 'requirements.txt', 'trading_agent']", 'stars': 79, 'watchers': 79, 'forks': 16, 'deprepos': 'zero', 'deppacks': 'zero'}