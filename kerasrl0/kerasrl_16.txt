{'dependent': 'kerasrl', 'repo': '/ntaylorwss/pavlov', 'language': 'Python', 'description': 'A modular, composable approach to reinforcement learning with a Keras backend and a functional data pipeline.', 'readme': '# Pavlov: A Modular, Composable Approach to Reinforcement Learning\n\nPavlov is an approach to reinforcement learning focused on a modular design. This design allows the building of reinforcement learning agents by selecting from among a set of options for each of the following components:\n\n- *Environment*. Any environment that complies with the OpenAI Gym interface can be used. Note that environments that do not comply with this interface cannot be used. For an example of how to register your custom environment as a Gym environment, see [Custom Environments](#custom-environments).\n- *State pipeline*. A pipeline is a sequence of pure functions that transform their input in some way. A state pipeline in Pavlov is defined as a list of functions. There is a collection of common functions in the module `pavlov.pipeline`, but you can easily write your own function and place it in the list. See [State Pipeline](#state-pipeline) for details.\n- *Model*. The model is the heart of reinforcement learning; this is where the actual learning takes place. For a full list of available models (being updated continuously), see [Models](#models).\n- *Actor*. The actor is responsible for converting the prediction of the Model into an action to be consumed by the Environment. This includes both exploration and conversion to the correct format for the environment\'s action space. The Actor will automatically and silently detect the type of action required by the environment (whether continuous, discrete, multi-dimensional discrete, multi-dimensional binary), and perform conversion according to the kind of model and action space it\'s working with. For more information on the kinds of exploration policies that are available, see [Exploration](#exploration).\n\nIt is based on Keras for model building, with a Tensorflow backend.\n\n## Installation\nPavlov is available in a Docker image that gives you all the tools you need in a simple and minimal environment. It includes such core components as Keras/Tensorflow, Numpy, Pandas, Matplotlib, as well as other helper libraries.\n\nAt the moment, Pavlov is only available for use within this image, and not as a standalone package. I\'ll be working on relieving this dependency.\n\n#### Getting the image\nThere exists two versions of this image: [this one](https://hub.docker.com/r/ntaylor22/pavlov-gpu/) for use with a GPU, and [this one](https://hub.docker.com/r/ntaylor22/pavlov-gpu/) for use without a GPU.\n\nIf you wish to use the GPU version, you must first install Nvidia Docker. The steps for installing Nvidia Docker are referenced below. Note that because of the dependency on Nvidia Docker, the GPU version can only be run from Linux variants, as Mac OSX and Windows are not currently supported by Nvidia Docker.\n\nOnce you have Nvidia-Docker (or not, if you\'re using the CPU version), to use this docker image, simply pull it from the repository:\n\n`docker pull ntaylor22/pavlov-gpu`\nOR:\n`docker pull ntaylor22/pavlov-cpu`\n\n#### Installing Nvidia Docker (for pavlov-gpu)\nInstallation instructions for Nvidia Docker can be found on their wiki [here](https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)).\n\nOne extra post-installation step is to create the file `/etc/docker/daemon.json`, with the following contents:\n```\n{\n    "default-runtime": "nvidia",\n    "runtimes": {\n        "nvidia": {\n            "path": "/usr/bin/nvidia-container-runtime",\n            "runtimeArgs": []\n        }\n    }\n}\n```\n\nThis will allow you to default to nvidia-docker, rather than using `nvidia-docker run` or the flag `--runtime=nvidia`.\n\n#### Running a container\nRunning a Docker container often requires setting a few flags. In this case, the main flags to be set are:\n\n- Required: `-p 8888:8888`: opening port 8888 of the container, which is where the Jupyter server is.\n- Required: `-p 6006:6006`: opening port 6006 of the container, which is where Tensorboard is.\n- Optional: `-v $PWD:/home`: mounting your working directory to /home, which is the working directory of the container.\n- Optional: `--name [container_name]`: setting the name of the container you\'re going to run.\n\nSo a standard command to launch a Pavlov container from a Unix system would be (assuming the GPU version of the image):\n\n`docker run --name pavlov -d --rm -p 8888:8888 -p 6006:6006 -v $PWD:/home ntaylor22/pavlov-gpu`\n\n## Getting Started\nHere\'s an example of an end-to-end usage of Pavlov to produce an agent for Breakout and have it run forever. Note that this agent doesn\'t actually solve Breakout with these settings, but once I figure out what settings solve it I will update the example.\n\n```python\nimport gym\nfrom keras import optimizers\n\nfrom pavlov import pipeline\nfrom pavlov import models\nfrom pavlov import exploration\nfrom pavlov import agents\n\nenv = gym.make(\'Breakout-v0\')\n\ntopology_config = {\n    \'layer_sizes\': [128],\n    \'activation\': \'relu\'\n}\ntopology = models.topology.DenseTopology(**topology_config)\n\ndqn_config = {\n    \'gamma\': 0.99,\n    \'tau\': 1.0,\n    \'optimizer\': optimizers.Adam(0.0001)\n}\nmodel = models.DQNModel(topology, **dqn_config)\n\nepsilon_schedule = auxiliary.schedules.LinearDecaySchedule(1.0, 0.1, 500, -1)\nactor = actors.EpsilonGreedyActor(epsilon_schedule)\nbuffer_size = 10000\nbatch_size = 64\n\npline = pipeline.Pipeline()\npline.add(transformations.rgb_to_grey())\npline.add(transformations.downsample(new_shape=(84, 84)))\npline.add(transformations.combine_consecutive(2, \'max\'))\npline.add(transformations.stack_consecutive(4))\n\nagent = agents.Agent(env,\n                     state_pipeline=pline,\n                     model=model, actor=actor,\n                     buffer_size=buffer_size, batch_size=batch_size,\n                     report_frequency=1, warmup_length=50)\nagent.run_indefinitely()\n```\n\n## Environments\n### Gym Environments\nPavlov is equipped to function with any environment that follows the [OpenAI Gym API](https://github.com/openai/gym/blob/master/gym/core.py#L11). For environments that are native to Gym, usage is straightforward. Simply use `gym.make` to generate an environment, and pass it to the Agent, as shown in [Getting Started](#getting-started).\n\n### Custom Environments\nA custom environment can surely be used with Pavlov, as long as it is first made to comply with the Gym API, and is properly registered as a Gym environment. What follows is all that is necessary to create a custom environment that complies with the OpenAI Gym API.\n\nA valid custom environment class must be necessarily equipped with the following _member variables_:\n\n- `self.observation_space`: An instance of a class from the `gym.spaces` module. Details are below under [Spaces](#spaces).\n- `self.action_space`: An instance of a class from the `gym.spaces` module. Details are below under [Spaces](#spaces).\n\nAnd it must be necessarily equipped with the following _methods_:\n\n- `self.seed(seed)`: It\'s not necessary for this to actually do anything if it\'s not needed, but it must be part of the class. A simple `return gym.utils.seeding.np_random(seed)[1:]` in the body of the function will work.\n- `self.render(mode, close)`: Again, not necessary for this to actually do anything if it doesn\'t make sense for your environment to render. In this event, you should use `return []` in the body as a dummy.\n- `self.reset()`: This is a key method, as it defines what happens at the termination of an episode. This should contain all your logic to reset the environment to its initial state.\n- `self.step(action)`: This is the main method, which defines a single timestep of the environment. It consumes an action and transitions the environment based on it.\n\nSee Gym documentation for more details on what these methods are for and what they\'re expected to return.\n\nFrom there, here is a snippet of slightly abstracted code (in the sense of the variable names being abstracted) for registering this environment with Gym:\n\n```python\nfrom gym.envs.registration import register\nfrom mymodule.mysubmodule import MyCustomEnvironment\n\ndef register_custom_env(env_init_arg_1, env_init_arg_n, env_name=\'MyCustomEnvironment\'):\n    kwargs = {\'env_init_arg_1\': env_init_arg_1, \'env_init_arg_n\': env_init_arg_n}\n    register(id=env_name, entry_point=\'mymodule.mysubmodule:MyCustomEnvironment\',\n             kwargs=kwargs)\n    return gym.make(env_name)\n```\n\nNote that an environment can only be registered once per Python session.\n\n### Spaces\nCurrently, Gym supports 4 types spaces for both observations and actions, found in the module `gym.spaces`: they are `Box`, `Discrete`, `MultiDiscrete`, and `MultiBinary`. All of these spaces are supported by Pavlov; though for Box, actions can only be up to 2D, while for observations, they can be any n-dimensional shape.\n\nKeep in mind when writing your environment that the output of any Space, including both the observation space and action space, must be the same at every timestep.\n\n## State Pipeline\nOften, a raw state is not going to be appropriate for effective learning, and transformations are required. The fundamental philosophy of this library is that data processing and feature engineering follows a "pipeline" structure. This means that a series of transformations are applied sequentially to each input state, to produce the final state formatting. These transformations should be implemented as individual functions. At that point, a list of these functions can be passed to the Agent. An example of a pipeline would be:\n\n```python\nfrom pavlov import pipeline\npipeline = [pipeline.rgb_to_grey(method=\'luminosity\'), # convert 3D RGB to 2D greyscale\n            pipeline.downsample(new_shape=(84, 84)), # resize image to smaller size by interpolation\n            pipeline.combine_consecutive(n_states=2, fun=\'max\'), # each state is the max of current and previous states\n            pipeline.stack_consecutive(n_states=4) # make each state a time-series of the last 4 states\n]\n```\n\nThe pipelined nature of this setup means that `combine_consecutive` and `stack_consecutive`, for example, can cooperate smoothly; `stack_consecutive` will make a stack of 4 combined states, since that\'s the output of the `combine_consecutive` step. The result of this whole pipeline is a stack of 4 84x84 greyscale max-outs over 2 consecutive frames, at each timestep.\n\nNote that the pipeline functions, such as `pipeline.rgb_to_grey()`, themselves return functions. `pipeline` is a list of functions. This design allows "hyperparameters" of functions, such as the shape of downsampling, to be easily specified.\n\n### Custom Functions\nThe only requirement for a pipeline function is that its signature takes `state` and `env` as arguments. Many pipeline functions will not make use of `env` information, but some will, so it\'s a necessary consistency. The recommendation is also to write pipeline functions as functors; that is, functions that return functions. As mentioned above, this allows easy specification and tuning of pipeline functions, and a readable syntax. Here\'s an example:\n\n```python\ndef reshape_array(new_shape):\n    def _reshape_array(state, env):\n        return np.reshape(state, new_shape)\n    return _reshape_array\n```\n\nThe convention to start the inner function with an underscore and name it identically to the outer function is just personal preference.\n\n## Models\nAt the moment, there are 2 reinforcement learning algorithms provided by Pavlov:\n\n- Deep Q Network (DQN)\n    - Double DQN is implemented, which is to say there is a target network; setting the parameter `tau` to `1.0` will effectively negate the target network and make it plain DQN\n- Deep Deterministic Policy Gradient (DDPG)\n\nThe parameters for each model are specified in the documentation and docstrings.\n\nOne core concept of Pavlov is that the feature extraction component of a neural network should be separated from the action selection component; for this reason, all classes for RL algorithms will expect to be given a headless Keras computation graph, or `pavlov.models.Topology`, as input.\n\n### Custom Model Configuration\nTo write an entirely customized Keras model graph, do the following:\n\n1. Start your new model class (let\'s call it `MyModel`), and have it inherit the `pavlov.models.topology.Topology` base class:\n    - e.g. `class MyModel(pavlov.models.topology.Topology):`\n2. Override the method `define_graph(self, ...)`, filling in `...` with whatever parameters your configuration requires, e.g. `dense_layer_sizes`. Alternatively, you can have no parameters:\n    - e.g. `def define_graph(self, dense_layer_sizes, activation, weight_initializer):`\n    - e.g. `def define_graph(self):`\n3. Initialize a new instance of your model class by passing your arguments for the parameters you provided in the declaration of `define_graph`:\n    - e.g. `model = MyModel(dense_layer_sizes=[16, 8, 4], activation=\'relu\', weight_initializer=\'glorot_normal\')`\n    - e.g. `model = MyModel()`\n\nWith this design, you can create any Keras graph you want, and parameterize it however you wish.\n\n### Input Layer\nA key note about this design is that the Input layer of the model is taken care of by the Agent internally. This layer will have the correct shape according to the output of your state pipeline, and it can be accessed through `self.input` in your child class of `Topology`. As an abbreviated example:\n\n```python\nclass MyCustomTopology(Topology):\n    def define_graph(self, layer1_size, ...):\n        X = Dense(layer1_size)(self.input) # accessing the Input layer through self.input\n        ...\n```\n\nThis way you do not have to care about making shapes agree whatsoever.\n\n## Exploration\nAt the moment, there is 1 method of exploration provided by Pavlov:\n\n- Epsilon Greedy\n\nFor any exploration module that requires an important value, such as the epsilon in epsilon greedy, there is a requirement to provide a schedule for that value\'s progression throughout training. Every schedule has a starting value, and will typically have an ending value, as well as other particular parameters. The schedules currently available are:\n\n- Constant. The value never changes.\n- Linear decay. The value decays linearly towards some point over some number of steps, and then flatlines.\n- Exponential decay. The value decays exponentially towards some point over some number of steps, and then asymptotes.\n- Scoping periodic. The value follows an exponential decay over some number of steps, but also oscillates in a sinusoidal wave, with specified period and amplitude. The result is a sinusoidal wave that eventually decays and flatlines.\n\n## Run indefinitely and interrupt cleanly\nAgents are equipped with a method to run episodes indefinitely; essentially an infinite loop of `agent.run_episode()`. This allows you to start an agent, and then walk away, to return at any point and find it continuing to run, without having to try to calculate how long a certain number of episodes may take.\n\nEventually, you\'re going to want to interrupt the agent\'s execution. Pavlov handles this by catching your KeyboardInterrupt, waiting until the completion of the current episode, then cleanly exiting before the start of the next episode. This way, you can run indefinitely and stop safely at any time.\n\n## TODO\n- A3C, PPO. That should round out the main RL algorithms.\n- Evolutionary strategies.\n- Parallel training and acting. Parallelism and concurrency in general.\n- More exploration policies.\n- Make Keras use multiple GPUs if available.\n- Hyperparameter optimization module.\n- Extend to multiple input sources, if Gym spaces allow. Might not be possible.\n', 'contents': "['.gitignore', 'HISTORY.rst', 'LICENSE', 'README.md', 'VERSION', 'docker', 'docs', 'notebooks', 'pavlov', 'release.sh', 'setup.py', 'tests']", 'stars': 2, 'watchers': 2, 'forks': 0, 'deprepos': 0, 'deppacks': 0}