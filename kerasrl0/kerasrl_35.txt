{'dependent': 'kerasrl', 'repo': '/KIC/pandas_ml_utils', 'language': 'Python', 'description': None, 'readme': '![PyPI - Downloads](https://img.shields.io/pypi/dw/pandas-ml-utils)\n![pandas-ml-utils.readthedocs.io](https://readthedocs.org/projects/pandas-ml-utils/badge/?version=latest&style=plastic\n)\n# Pandas ML Utils\n\nPandas ML Utils is intended to help you through your journey of statistical or machine learning models, \nwhile you never need to leave the world of pandas.\n\n 1. install `pip install pandas-ml-utils`\n    1. optional finance: `pip install pandas-ml-utils[finance]` allows you to `pd.fetch_yahoo(...)`\n    1. optional crypto: `pip install pandas-ml-utils[crypto]` allows you to `pd.fetch_crypto(...)`\n    1. optional notebook: `pip install pandas-ml-utils[notebook]` renders results nicely in notebooks \n    1. optional development: `pip install pandas-ml-utils[development]` if you want to develop \n 1. analyze your features\n 1. find a model\n 1. save and reuse your model\n\nOr [read the docs](https://pandas-ml-utils.readthedocs.io/en/latest/).\n\n## Install\n```bash\npip install pandas-ml-utils\n```\n\n## Analyze your Features\n\nThe feature_selection functionality helps you to analyze your features, filter out highly correlated once and focus on the most important features. This function also applies an auto regression and embeds and ACF plot.\n\n\n```python\nimport pandas_ml_utils as pmu\nimport pandas as pd\n\ndf = pd.read_csv(\'burritos.csv\')[["Tortilla", "Temp", "Meat", "Fillings", "Meat:filling", "Uniformity", "Salsa", "Synergy", "Wrap", "overall"]]\ndf.feature_selection(label_column="overall")\n\n```\n\n\n![png](Readme_files/Readme_2_0.png)\n\n\n              Tortilla   overall   Synergy  Fillings      Temp     Salsa  \\\n    Tortilla       1.0  0.403981  0.367575  0.345613  0.290702  0.267212   \n    \n                  Meat  Uniformity  Meat:filling      Wrap  \n    Tortilla  0.260194    0.208666      0.207518  0.160831  \n    label is continuous: True\n\n\n\n![png](Readme_files/Readme_2_2.png)\n\n\n    Feature ranking:\n    [\'Synergy\', \'Meat\', \'Fillings\', \'Meat:filling\', \'Wrap\', \'Tortilla\', \'Uniformity\', \'Salsa\', \'Temp\']\n    \n    TOP 5 features\n             Synergy      Meat  Fillings  Meat:filling     Wrap\n    Synergy      1.0  0.601545  0.663328      0.428505  0.08685\n    \n    filtered features with correlation < 0.5\n               Synergy  Meat:filling      Wrap\n    Tortilla  0.367575      0.207518  0.160831\n\n\n\n![png](Readme_files/Readme_2_4.png)\n\n\n\n![png](Readme_files/Readme_2_5.png)\n\n\n    Synergy       1.000000\n    Synergy_0     1.000000\n    Synergy_1     0.147495\n    Synergy_56    0.128449\n    Synergy_78    0.119272\n    Synergy_55    0.111832\n    Synergy_79    0.086466\n    Synergy_47    0.085117\n    Synergy_53    0.084786\n    Synergy_37    0.084312\n    Name: Synergy, dtype: float64\n\n\n\n![png](Readme_files/Readme_2_7.png)\n\n\n    Meat:filling       1.000000\n    Meat:filling_0     1.000000\n    Meat:filling_15    0.185946\n    Meat:filling_35    0.175837\n    Meat:filling_1     0.122546\n    Meat:filling_87    0.118597\n    Meat:filling_33    0.112875\n    Meat:filling_73    0.103090\n    Meat:filling_72    0.103054\n    Meat:filling_71    0.089437\n    Name: Meat:filling, dtype: float64\n\n\n\n![png](Readme_files/Readme_2_9.png)\n\n\n    Wrap       1.000000\n    Wrap_0     1.000000\n    Wrap_63    0.210823\n    Wrap_88    0.189735\n    Wrap_1     0.169132\n    Wrap_87    0.166502\n    Wrap_66    0.146689\n    Wrap_89    0.141822\n    Wrap_74    0.120047\n    Wrap_11    0.115095\n    Name: Wrap, dtype: float64\n    best lags are\n    [(1, \'-1.00\'), (2, \'-0.15\'), (88, \'-0.10\'), (64, \'-0.07\'), (19, \'-0.07\'), (89, \'-0.06\'), (36, \'-0.05\'), (43, \'-0.05\'), (16, \'-0.05\'), (68, \'-0.04\'), (90, \'-0.04\'), (87, \'-0.04\'), (3, \'-0.03\'), (20, \'-0.03\'), (59, \'-0.03\'), (75, \'-0.03\'), (91, \'-0.03\'), (57, \'-0.03\'), (46, \'-0.02\'), (48, \'-0.02\'), (54, \'-0.02\'), (73, \'-0.02\'), (25, \'-0.02\'), (79, \'-0.02\'), (76, \'-0.02\'), (37, \'-0.02\'), (71, \'-0.02\'), (15, \'-0.02\'), (49, \'-0.02\'), (12, \'-0.02\'), (65, \'-0.02\'), (40, \'-0.02\'), (24, \'-0.02\'), (78, \'-0.02\'), (53, \'-0.02\'), (8, \'-0.02\'), (44, \'-0.01\'), (45, \'0.01\'), (56, \'0.01\'), (26, \'0.01\'), (82, \'0.01\'), (77, \'0.02\'), (22, \'0.02\'), (83, \'0.02\'), (11, \'0.02\'), (66, \'0.02\'), (31, \'0.02\'), (80, \'0.02\'), (92, \'0.02\'), (39, \'0.03\'), (27, \'0.03\'), (70, \'0.04\'), (41, \'0.04\'), (51, \'0.04\'), (4, \'0.04\'), (7, \'0.05\'), (13, \'0.05\'), (97, \'0.06\'), (60, \'0.06\'), (42, \'0.06\'), (96, \'0.06\'), (95, \'0.06\'), (30, \'0.07\'), (81, \'0.07\'), (52, \'0.07\'), (9, \'0.07\'), (61, \'0.07\'), (84, \'0.07\'), (29, \'0.08\'), (94, \'0.08\'), (28, \'0.11\')]\n\n\n## Fit a Model\nOnce you know your features you can start to try out different models i.e. a very basic\nLogistic Regression. It is also possible to search through a set of hyper parameters. \n\n\n```python\nimport pandas as pd\nimport pandas_ml_utils as pmu\nfrom sklearn.linear_model import LogisticRegression\nfrom pandas_ml_utils.summary.binary_classification_summary import BinaryClassificationSummary\n\ndf = pd.read_csv(\'burritos.csv\')\ncolumns = ["Tortilla", "Temp", "Meat", "Fillings", "Meat:filling", "Uniformity", "Salsa", "Synergy", "Wrap", "overall", "with_fires", "price"]\nfit = df.fitpmu.SkitModel(LogisticRegression(solver=\'lbfgs\'),\n                          pmu.FeaturesAndLabels(["Tortilla", "Temp", "Meat", "Fillings", "Meat:filling",\n                                                  "Uniformity", "Salsa", "Synergy", "Wrap", "overall"],\n                                                 ["with_fires"], \n                                                pre_processor=lambda _df: pmu.LazyDataFrame(_df,\n                                                                                            with_fires = lambda f: f["Fries"].apply(lambda x: str(x).lower() == "x"),\n                                                                                            price      = lambda f: f["Cost"] * -1).to_dataframe()[columns].dropna()),\n                          BinaryClassificationSummary)\n\nfit\n```\n\n![png](Readme_files/fit_burritos.png)\n\n\n## Save and use your model\nOnce you are happy with your model you can save it and apply it on any DataFrame which\nserves the needed columns by your features.\n\n```python\nfit.save_model("/tmp/burrito.model")\n```\n\nAn then just apply the model on the data frame as you would expect it from your data source:\n\n```python\ndf = pd.read_csv(\'burritos.csv\')\ndf.predict(pmu.Model.load("/tmp/burrito.model")).tail()\n```\n\n<div>\n<table border="1" class="dataframe">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan="3" halign="left">price</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th colspan="2" halign="left">prediction</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>value</th>\n      <th>value_proba</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>380</th>\n      <td>False</td>\n      <td>0.251311</td>\n      <td>-6.85</td>\n    </tr>\n    <tr>\n      <th>381</th>\n      <td>False</td>\n      <td>0.328659</td>\n      <td>-6.85</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>False</td>\n      <td>0.064751</td>\n      <td>-11.50</td>\n    </tr>\n    <tr>\n      <th>383</th>\n      <td>False</td>\n      <td>0.428745</td>\n      <td>-7.89</td>\n    </tr>\n    <tr>\n      <th>384</th>\n      <td>False</td>\n      <td>0.265546</td>\n      <td>-7.89</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n## TODO\n* allow multiple class for classification \n* replace hard coded summary objects by a summary provider function \n* add more tests\n* add Proximity https://stats.stackexchange.com/questions/270201/pooling-levels-of-categorical-variables-for-regression-trees/275867#275867\n\n## Wanna help?\n* for tensorflow 2.x implement a new TfKeras Model\n* for non classification problems you might want to augment the `Summary` \n* write some tests\n* add different more charts for a better understanding/interpretation of the models\n* add whatever you need for yourself and share it with us \n\n## Change Log\n### 0.0.25 / 26\n* refactored how traing and test data sets are split\n* allow to control the amount of young test data being used (useful for time series)\n* add sample weights i.e. to penalize loss per sample in a keras model \n \n### 0.0.23 / 24\n* changed SkitModel to SkModel\n* some minor bug fixes  \n\n### 0.0.22\n* introduce proper keras session and graph handling in case of tensorflow backend\n* rename features_and_labels.loss to gross_loss to avoid confusion with traning loss\n\n### 0.0.21\n* added engineered source frame to backtest\n* introduced pre-processing of data frame in features and labels\n* changed the lambda parameters of target and loss providers (can be 1, 2 or 3 parameter lambda)\n* bugfixes in laza dataframe\n \n### 0.0.18\n* refactored the data frame logic in the feature and label extractor for using multi level index\n\n### 0.0.16, 0.0.17\n* there is now only one `fit` and only one `backtest` and `predict` method\n* Summary class has to be provided as part of the model i.e. BinaryClassificationSummary\n\n### 0.0.12\n* added sphinx documentation\n* added multi model as regular model which has quite a big impact\n  * features and labels signature changed\n  * multiple targets has now the consequence that a lot of things a returning a dict now\n  * everything is using now DataFrames instead of arrays after plain model invoke\n* added some tests\n* fixed some bugs a long the way\n\n### 0.0.11\n* Added Hyper parameter tuning \n```python\nfrom hyperopt import hp\n\nfit = df.fit_classifier(\n            pdu.SkitModel(MLPClassifier(activation=\'tanh\', hidden_layer_sizes=(60, 50), random_state=42),\n                          pdu.FeaturesAndLabels(features=[\'vix_Close\'], labels=[\'label\'],\n                                                targets=("vix_Open", "spy_Volume"))),\n            test_size=0.4,\n            test_validate_split_seed=42,\n            hyper_parameter_space={\'alpha\': hp.choice(\'alpha\', [0.001, 0.1]), \'early_stopping\': True, \'max_iter\': 50,\n                                   \'__max_evals\': 4, \'__rstate\': np.random.RandomState(42)})\n```\nNOTE there is currently a bug in hyperot [module bson has no attribute BSON](https://github.com/hyperopt/hyperopt/issues/547)\n! However there is a workaround:\n```bash\nsudo pip uninstall bson\npip install pymongo\n``` \n\n### 0.0.10\n* Added support for rescaling features within the auto regressive lags. The following example\nre-scales the domain of min/max(featureA and featureB) to the range of -1 and 1. \n```python\nFeaturesAndLabels(["featureA", "featureB", "featureC"],\n                  ["labelA"],\n                  feature_rescaling={("featureA", "featureC"): (-1, 1)})\n```\n* added a feature selection functionality. When starting from scratch this just helps\nto analyze the data to find feature importance and feature (auto) correlation.\nI.e. `df.filtration(label_column=\'delta\')` takes all columns as features exept for the\ndelta column (which is the label) and reduces the feature space by some heuristics.\n', 'contents': "['.codacy.yml', '.github', '.gitignore', '.readthedocs.yml', 'LICENSE', 'README.md', 'Readme_files', 'deploy.sh', 'docs-requirements.txt', 'docs', 'pandas_ml_utils', 'pyproject.toml', 'requirements.txt', 'test']", 'stars': 1, 'watchers': 1, 'forks': 0, 'deprepos': 0, 'deppacks': 0}