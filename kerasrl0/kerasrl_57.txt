{'dependent': 'kerasrl', 'repo': '/christianhidber/easyagents', 'language': 'Jupyter Notebook', 'description': 'Reinforcement Learning for Practitioners.', 'readme': '### Reinforcement Learning for Practitioners (v1.4.1, 20Q1)\n![Travis_Status](https://travis-ci.com/christianhidber/easyagents.svg?branch=master)\n[![Coverage Status](https://coveralls.io/repos/github/christianhidber/easyagents/badge.svg?branch=master)](https://coveralls.io/github/christianhidber/easyagents?branch=master)\n![License](https://img.shields.io/github/license/christianhidber/easyagents)\n[![Downloads](https://pepy.tech/badge/easyagents)](https://pepy.tech/project/easyagents)\n[![Docs](https://img.shields.io/badge/api-docs-blue)](https://christianhidber.github.io/easyagents/)\n\n\nStatus: \nunder active development, breaking changes may occur. \n[Release notes](documentation/Markdown/Release_Notes.md).\n\n![EasyAgents logo](images/EazyAgentsIcon.png)\n\nEasyAgents is a high level reinforcement learning api focusing on ease of use and simplicity.\nWritten in Python and running on top of established reinforcement learning libraries like\n[tf-Agents](https://github.com/tensorflow/agents), \n[tensorforce](https://github.com/tensorforce/tensorforce) or \n[keras-rl](https://github.com/keras-rl/keras-rl).\nEnvironments are implemented in [OpenAI gym](https://github.com/openai/gym). \nFor an example of an industrial application of reinforcement learning see [here](#Industrial-Application).\n\nIn collaboration with [Oliver Zeigermann](http://zeigermann.eu/). \n\n### Features \n---\n* provides the **same, simple api across all libraries**. Thus you can easily switch between different implementations\n  and you don\'t have to learn for each of them a new api.\n* to create and run any algorithm you only need **2 lines of code**, all the parameters are named\n  consistently across all algorithms.\n* supports a broad set of different algorithms\n* runs inside **jupyter notebooks** as well as stand-alone, easy to install requiring only a single \n  \'pip install easyagents\'.\n* easy to understand, **ready-made plots** and logs to investigate the algorithms and environments behaviour\n\n### Examples\n---\n````\nfrom easyagents.agents import PpoAgent\nfrom easyagents.callbacks import plot\n\nppoAgent = PpoAgent(\'CartPole-v0\')\nppoAgent.train([plot.State(), plot.Loss(), plot.Rewards()])\n````\n![Scenario_Simple](images/Scenario_simple.png)\n\n#### More Detailed\n````\nfrom easyagents.agents import PpoAgent\nfrom easyagents.callbacks import plot\n\nppoAgent = PpoAgent( \'Orso-v1\',fc_layers=(500,500,500))\nppoAgent.train([plot.State(), plot.Loss(), plot.Rewards(), plot.Actions(), \n                plot.StepRewards(), plot.Steps(), plot.ToMovie()], \n                learning_rate = 0.0001, num_iterations = 500, max_steps_per_episode=50 )\n````\n\n![Scenario_Detailed](images/Scenario_detailed.gif)\n\n### Tutorials\n---\n* [1. Introduction (CartPole on colab)](https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/intro_cartpole.ipynb):\n   training, plotting, switching algorithms & backends. Based on the classic reinforcement learning example \n   balancing a stick on a cart.\n* [2. Next steps & backend switching (Orso on colab)](https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/intro_orso.ipynb):\n  custom training, creating a movie & switching backends. gym environment based on a routing problem.\n* [3. Controlling training & evaluation (on colab)](https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/intro_train_args.ipynb):\n   or \'what do all these agent.train(...) args mean ?\'\n* [4. Creating your own environment (LineWorld on colab)](https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/intro_line.ipynb):\n  implement a gym environment from scratch, workshop example.\n* [5. Saving & loading (on colab)](https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/intro_persistence.ipynb): \n  Once a policy is trained, save it and reload it in a production environment. \n  You may also save intermediate policies as the training proceeds. \n* [6. Switching backends (on colab)](https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/intro_switching_backends.ipynb):\n  See how you can switch between backend implementations.\n* [7. Api logging, seeding & plot clearing (on colab)](https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/intro_logging.ipynb): \n  Investigate how easyagent interacts with the backend api and the gym environment; \n  how to set seeds; controlling jupyter output cell clearing\n\n### Available Algorithms and Backends\n---\n\n|algorithm | [tf-Agents](https://github.com/tensorflow/agents) | [tensorforce](https://github.com/tensorforce/tensorforce) | [keras-rl (suspended)](https://github.com/keras-rl/keras-rl) | easyagents class name |\n|----------|:---------:|:-----------:|:--------:| :---: | \n|[CEM](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf) |`not available`  |`not available`  |`yes`  | CemAgent | \n|[Dqn](https://arxiv.org/abs/1312.5602)           |`yes`            |`yes`    |`yes`            | DqnAgent | \n|[Double Dqn](https://arxiv.org/abs/1509.06461)   |`open`           |`not available`    |`yes`  | DoubleDqnAgent|\n|[Dueling Dqn](https://arxiv.org/abs/1511.06581)  |`not available`  |`yes`    |`yes`            | DuelingDqnAgent|\n|[Ppo](https://arxiv.org/abs/1707.06347)          |`yes`            |`yes`    |`not available`  | PpoAgent |\n|Random                                           |`yes`            |`yes`    |`not available`  | RandomAgent |\n|[REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)  |`yes`  |`yes` |`not available`| ReinforceAgent | \n|[SAC](https://arxiv.org/abs/1801.01290)          |`preview`        |`not available` |`not available`|SacAgent|\n\n[191001]\n\n* if you are interested in other algorithms, backends or hyperparameters let us know by\n [creating an issue](https://github.com/christianhidber/easyagents/issues/new/choose). \n  We\'ll try our best to support you.\n* for a documentation of the agents api see [here](https://christianhidber.github.io/easyagents/).\n* starting with easyagents 1.3 (191102) the backend for keras-rl is suspended \n  until support for tensorflow 2.0 is available.\n\n### Industrial Application\n---\n[Geberit](https://www.geberit.com/en/) - a sanitary technology company with > 12\'000 employees -\nproduces in particular pipes and other parts to get rain-water of flat roofs - so called\n[syphonic roof drainage systems](https://www.international.geberit.com/products/piping-systems-drainage/geberit-pluvia-roof-drainage/). \nThey warrant that large buildings like stadiums, airports or shopping malls do not collapse during \nheavy rainfalls. \nHowever it is surprisingly difficult to find the right dimensions for the pipes. \nIt is actually so difficult, that as of today **no feasable, deterministic algorithm** is known. \nThus traditional heuristics and classic machine learning were used to support the users \nin finding a suitable solution.\n\nUsing reinforcement learning the failrate of the previous solution was reduced by 70%, resulting\nin an end-to-end success-rate of > 98%.\n\n<p align="center"> \n<img src="https://raw.githubusercontent.com/christianhidber/easyagents/master/images/Pluvia_small.png">\n</p>\n\nFor more details take a look at this [talk](https://www.youtube.com/watch?v=3RjSanoNIlk).\n\n\n### Installation\n---\nInstall from pypi using pip:\n\n```python\npip install easyagents\n```\n\n\n### More\n---\n#### Documentation\nfor [release notes & class diagram](documentation/Markdown/Release_Notes.md), \nfor [agents & api](https://christianhidber.github.io/easyagents/). \n\n#### Guiding Principles\n* easily train, evaluate & debug policies for (you own) gym environment over "designing new algorithms"\n* simple & consistent over "flexible & powerful"\n* inspired by keras: \n    * same api across all algorithms\n    * support different implementations of the same algorithm \n    * extensible (pluggable backends, plots & training schemes)   \n\n#### EasyAgents may not be ideal if\n\n* you would like to leverage implementation specific advantages of an algorithm\n* you want to do distributed or in parallel reinforcement learning\n\n#### Note\n\n* If you have any difficulties in installing or using easyagents please let us know by \n  [creating an issue](https://github.com/christianhidber/easyagents/issues/new/choose).\n  We\'ll try our best to help you.\n* Any ideas, help, suggestions, comments etc in python / open source development / reinforcement learning / whatever\n  are more than welcome. \n', 'contents': "['.coveragerc', '.github', '.gitignore', '.travis.yml', 'CODE_OF_CONDUCT.md', 'CONTRIBUTING.md', 'LICENSE', 'README.md', 'documentation', 'easyagents', 'images', 'jupyter_notebooks', 'poster', 'pyproject.toml', 'requirements.txt', 'setup.py', 'tools']", 'stars': 23, 'watchers': 23, 'forks': 16, 'deprepos': '0', 'deppacks': '0'}