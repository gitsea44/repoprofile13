{'dependent': 'kerasrl', 'repo': '/furfa/LunarLander-v2-Solve', 'language': 'Python', 'description': 'MTS contest', 'readme': '# LunarLander-v2-Solve (MTS contest) BEST SCORE: 231\n\n![preview](https://github.com/furfa/LunarLander-v2-Solve/blob/master/img/preview.gif)\n\n> Награда за переход от верхней части экрана к посадочной площадке и нулевой скорости составляет около 100..140 баллов. Если посадочный аппарат отходит от посадочной площадки, он теряет награду. Эпизод заканчивается, если посадочный модуль падает или останавливается, получая дополнительные -100 или +100 очков. Каждый контакт с землей оценивается в +10 очков. Использование главного двигателя стоит -0,3 балла за каждый кадр. Выполнение условия посадки дает дополнительные 200 баллов. Возможна посадка вне посадочной площадки. Топливо бесконечно, поэтому агент может научиться летать, а затем научиться приземляться. Доступны четыре отдельных действия: ничего не делать, запустить двигатель с левой ориентацией, главный двигатель, двигатель с правой ориентацией.\n## Метрика\n> LunarLander-v2 определяет «решение» как получение среднего вознаграждения 200 за 100 последовательных испытаний.\n\n## Использованные нами библиотеки:\n* pytorch\n* numpy\n* gym\n* pickle (Для сохранения агентов)\n\n## Что мы делали\n* За бейзлайн взяли модель и реализацию DQN отсюда [ТЫЦ](https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/DeepQLearning) (По факту от нее уже ничего не осталось), и адаптировали к нашей задаче. Получили ответ более чем за 1000 итераций. Также посмотрели решение второго места лидерборда [ТЫЦ](https://github.com/plopd/deep-reinforcement-learning/blob/master/dqn/Deep_Q_Network.ipynb)\n* Далее изменили алгоритм обновления весов фиксированной модели. Вместо простого перекидывания, стали взвешивать. Это дало лучший скор в 800 итераций\n```python\nif self.replace_target_cnt is not None and \\\n    self.learn_step_counter % self.replace_target_cnt == 0:\n\n    self.Q_next.load_state_dict(self.Q_eval.state_dict())\n```\n\n```python\ndef update_weight(self, model_from, model_to, tau):\n        for from_p, to_p in zip(model_from.parameters(), model_to.parameters()):\n            to_p.data.copy_(tau*from_p.data + (1.0-tau)*to_p.data)\n```\n* Переписали память на numpy массивы - это дало серьезный прирост к скорости обучения. И инициализировали ее размером бача(Делая случайные действия в среде).\n* Далее начался тюнинг, самым значимым параметром был EPSILON (он контролировал начальное изучение среды), определяющий вероятность совершения случайного действия, также мы минимизировали его разными методами, т.е. линейно, экспоненциально и т.д. После оптимизации задача решалась за +- 600 итераций.\n* В нашей задаче максимальный reward равен 100 и даётся он при успешной посадке.Наш алгоритм пытается получить максимальный Ревард за итерацию и начинает летать, пытаясь получить ревард за успешную посадку ещё раз.Таким образом, чтобы избежать потерь reward’ов \nза использования основного двигателя, мы заканчиваем эпизод, если наш «Луноход» приземлился на землю. Это улучшило скор до 550 эпизодов и сильно сократило время обучения.\n```python\ndef kostil(reward):\n    return (\n        reward == 100 or \n        reward == -100 or \n        reward == 10 or \n        reward == 200\n        )\n```\n* Чтобы смотреть, как обучается модель мы написали класс для временного включения визуализации из gymа, \n```python\ndef try_block(env, scores, pbar, visualize):\n    try:\n        main_loop(env, scores, pbar, visualize)\n    except KeyboardInterrupt:\n        inp = input("""\n            o - Остановка обучения,\n            +v - Включить визуализацию\n            -v - Выключить визуализацию\n        """)\n        if inp == \'o\':\n            print("Остановка обучения.")\n        elif inp == \'+v\':\n            try_block(env, scores, pbar, True)\n        elif inp == \'-v\':\n            try_block(env, scores, pbar, False)\n        else:\n            print("Продолжаем :)")\n            try_block(env, scores, pbar, visualize)\n```\nНапример в данной ситуации модель переобучилась, т.к. mean score растет медленно, а модель пытается адаптироваться ко всем observationам.\n![alt](https://github.com/furfa/LunarLander-v2-Solve/blob/master/img/605.png)\n\nСитуация исправляется хорошей начальной инициализацией памяти, и уменьшением скорости уменьшения epsilona (Добавлением большего количества рандомных действий). Это все позволяет лучше исследовать среду. \n\nПосле оптимизации: \n![alt](https://github.com/furfa/LunarLander-v2-Solve/blob/master/img/411.jpg)\n\n* Далее мы последовали советам https://drive.google.com/file/d/0BxXI_RttTZAhVUhpbDhiSUFFNjg/view из презентации от DEEPMIND.\n\nИзменили MSELOSS на HUBER.\n\nПодобрали гиперпараметры и получили скор 231.\n\nТак выглядит финальная модель.\n\n```python\nclass HuberNet(nn.Module):\n    def __init__(self, ALPHA, INPUT_SHAPE, OUTPUT_SHAPE):\n        super().__init__()\n\n        self.ALPHA = ALPHA\n\n        self.model = nn.Sequential(\n            nn.Linear(INPUT_SHAPE, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, OUTPUT_SHAPE),\n        )\n\n        # self.optimizer = optim.RMSprop(self.parameters(), lr=self.ALPHA, momentum=0.0001) # Tune this\n        self.optimizer = optim.Adam(self.parameters(), lr=self.ALPHA) # Tune this\n\n        self.loss = nn.SmoothL1Loss()\n        self.device = torch.device(\'cuda:0\' if torch.cuda.is_available() else \'cpu\')     \n        self.to(self.device)\n\n    def forward(self, observation):\n        observation = torch.Tensor(observation).to(self.device)\n        actions = self.model(observation)\n        return actions\n```\n\n## Результаты:\n\n![alt](https://github.com/furfa/LunarLander-v2-Solve/blob/master/img/231.jpg)\n\n## Наилучший результат - 231 итерация.\n\n### ./Scripts/MAIN_BEST.py ', 'contents': "['.gitignore', 'LICENSE', 'Notebooks', 'README.md', 'Results', 'Scripts', 'TorchExample', 'activate.sh', 'environment.yml', 'gym', 'img', 'requirements.txt', 'save_libs.sh', 'src', 'ПРЕЗЕНТАЦИЯ.pdf']", 'stars': 6, 'watchers': 6, 'forks': 0, 'deprepos': 'zero', 'deppacks': 'zero'}