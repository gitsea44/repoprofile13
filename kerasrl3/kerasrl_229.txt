{'dependent': 'kerasrl', 'repo': '/Rowing0914/TF2_RL', 'language': 'Python', 'description': 'Eagerly Experimentable!!!', 'readme': '\n<p align="center"><img src="./assets/logo1.png"></p>\n## TF-RL2(Reinforcement Learning with Tensorflow: EAGER!!)\n\n[![](https://img.shields.io/badge/TensorFlow-v2.0-blue)](https://github.com/Rowing0914/TF2_RL) [![](https://img.shields.io/badge/Platform-Google%20Colab-lightgrey)](https://github.com/Rowing0914/TF2_RL)\n\nThis is the repo for implementing and experimenting the variety of RL algorithms using **Tensorflow Eager Execution**. And, since our Lord Google gracefully allows us to use their precious GPU resources without almost restriction, I have decided to enable most of codes run on **Google Colab**. So, if you don\'t have GPUs, please feel free to try it out on **Google Colab**\n\nNote: As it is known that **Eager mode** is slower than **Graph Execution** in execution time so that I am using **Eager** for debugging and **Graph** mode for training!! How is it possible?? `@tf.function` allows us to render an annotated function to the **computational graph** at execution.\n\n\n\n## Installation\n\n```shell\ngit clone https://github.com/Rowing0914/TF_RL2.git\ncd TF_RL\npython setup.py install\n```\n\n\n\n## Features\n\n- Ready-to-run on Google colab( [Result of DQN](https://github.com/Rowing0914/TF_RL/blob/master/result/DQN/README.md))\n\n```shell\n# you can run on google colab, but make sure that there some restriction on session\n# 1. 90 minutes session reflesh\n# 2. 12 Hours session reflesh\n# Assuming you execute cmds below on Google Colab Jupyter Notebook\n$ !git clone https://github.com/Rowing0914/TF_RL.git\n$ pip install --index-url https://test.pypi.org/simple/ --no-deps TF_RL\n$ %cd TF_RL\n$ python3.6 examples/{model_name}/{model_name}_eager_atari.py --mode Atari --env_name={env_name} --google_colab=True\n\n# === Execute On Your Local Machine ===\n# My dirty workaroud to avoid breaking the connection to Colab is to execute below on local PC\n$ watch -n 3600 python3.6 {your_filename}.py\n\n""" Save this code to {your_filename}.py\nimport pyautogui\nimport time\n\n# terminal -> chrome or whatever\npyautogui.hotkey("alt", "tab")\ntime.sleep(0.5)\n# reflesh a page\npyautogui.hotkey("ctrl", "r")\ntime.sleep(1)\n# say "YES" to a confirmation dialogue\npyautogui.hotkey("Enter")\ntime.sleep(1)\n# next page\npyautogui.hotkey("ctrl", "tab")\n# check all page reload properly\npyautogui.hotkey("ctrl", "tab")\ntime.sleep(1)\n# switch back to terminal\npyautogui.hotkey("alt", "tab")\ntime.sleep(0.5)\n"""\n```\n\n\n\n## Implementations\n\n- Please check `tf_rl/examples`, each directory contains its own `README` so please follow it as well!!\n\n- I\'ve tried some implementations from R.Sutton\'s Great Book!\n\n\n\n\n## Game Envs\n\n### Atari Envs\n\n```python\nfrom tf_rl.common.wrappers import wrap_deepmind, make_atari\nfrom tf_rl.common.params import ENV_LIST_NATURE, ENV_LIST_NIPS\n\n\n# for env_name in ENV_LIST_NIPS:\nfor env_name in ENV_LIST_NATURE:\n    env = wrap_deepmind(make_atari(env_name))\n    state = env.reset()\n    for t in range(10):\n        # env.render()\n        action = env.action_space.sample()\n        next_state, reward, done, info = env.step(action)\n        # print(reward, next_state)\n        state = next_state\n        if done:\n            break\n    print("{}: Episode finished after {} timesteps".format(env_name, t + 1))\n    env.close()\n```\n\n### Atari Env with Revertable Wrapper\n[[Youtube Demo]](https://www.youtube.com/watch?v=dAo2jn7ElLk&feature=youtu.be)\n\n```python\nimport time, gym\nfrom tf_rl.common.wrappers import wrap_deepmind, make_atari, ReplayResetEnv\n\nenv = wrap_deepmind(make_atari("PongNoFrameskip-v4"))\nenv = gym.wrappers.Monitor(env, "./video")\nenv = ReplayResetEnv(env)\n\nstate = env.reset()\n\nfor t in range(1, 1000):\n    env.render()\n    action = env.action_space.sample()\n    next_state, reward, done, info = env.step(action)\n    state = next_state\n\n    if t == 300:\n        time.sleep(0.5)\n        recover_state = env.get_checkpoint_state()\n\n    if (t > 300) and (t % 100 == 0):\n        env.recover(recover_state)\n        env.step(0)  # 1 extra step to burn the current state on ALE\'s RAM is required!!\n        env.render()\n        time.sleep(0.5)\n\nenv.close()\n```\n\n### CartPole-Pixel(Obs: Raw Pixels in NumpyArray)\n\n```python\nimport gym\nfrom tf_rl.common.wrappers import CartPole_Pixel\n\nenv = CartPole_Pixel(gym.make(\'CartPole-v0\'))\nfor ep in range(2):\n\tenv.reset()\n\tfor t in range(100):\n\t\to, r, done, _ = env.step(env.action_space.sample())\n\t\tprint(o.shape)\n\t\tif done:\n\t\t\tbreak\nenv.close()\n```\n\n### MuJoCo(pls, check the MuJoCo official repo for more details...)\n\n```python\n# run this from the terminal and make sure you are loading appropriate environment variables\n# $ echo $LD_LIBRARY_PATH\n\nimport gym\nfrom tf_rl.common.params import DDPG_ENV_LIST\n\nfor env_name, goal_score in DDPG_ENV_LIST.items():\n\tenv = gym.make(env_name)\n\tenv.reset()\n\tfor _ in range(100):\n\t\tenv.render()\n\t\tenv.step(env.action_space.sample()) # take a random action\n```\n\n### MuJoCo Humanoid Maze\nhttps://github.com/Rowing0914/MuJoCo_Humanoid_Maze\n\n```python\nimport gym\nimport humanoid_maze # this is the external library(check the link above!!)\n\nenv = gym.make(\'HumanoidMaze-v0\')\n\nenv.reset()\nfor _ in range(2000):\n    env.render()\n    env.step(env.action_space.sample()) # take a random action\nenv.close()\n\n```\n\nI have contributed to this project as well.\nhttps://github.com/Breakend/gym-extensions\n```python\nimport gym\nfrom gym_extensions.continuous import mujoco\n\n# available env list: https://github.com/Rowing0914/gym-extensions/blob/mujoco200/tests/all_tests.py\nenv = gym.make("PusherMovingGoal-v1")\n\nenv.reset()\nfor _ in range(100):\n    env.render()\n    s, r, d, i = env.step(env.action_space.sample()) # take a random action\n    print(s.shape, r, d, i)\nenv.close()\n\n```\n\n\n## PC Envs\n\n- OS: Linux Ubuntu LTS 18.04\n- Python: 3.x\n- GPU: NVIDIA RTX 2080 Max Q Design\n- Tensorflow: 2.0.0\n\n\n\n### GPU Installation Support\n\n- Check [this link](https://www.tensorflow.org/install/gpu)\n- if you encounter some error related to the `unmet dependencies`, pls check [this link](https://devtalk.nvidia.com/default/topic/1043184/cuda-setup-and-installation/cuda-install-unmet-dependencies-cuda-depends-cuda-10-0-gt-10-0-130-but-it-is-not-going-to-be-installed/post/5362151/#5362151)\n\n\n\n## References\n\n- [Logomaker](<https://www.logaster.co.uk/?_ga=2.128584591.2087808828.1559775482-1265517291.1559775482>)\n- if you get stuck at DQN, you may want to refer to this great guy\'s entry: <https://adgefficiency.com/dqn-debugging/>\n', 'contents': "['.gitignore', 'LICENSE', 'README.md', 'Tips_Atari_Env.md', 'assets', 'init_server.sh', 'mujoco_what_is_qpos_qvel.md', 'requirements.txt', 'setup.py', 'tf_rl']", 'stars': 2, 'watchers': 2, 'forks': 0, 'deprepos': 0, 'deppacks': 0}