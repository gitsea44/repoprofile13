{'dependent': 'kerasrl', 'repo': '/MishaLaskin/vqvae', 'language': 'Jupyter Notebook', 'description': 'A pytorch implementation of the vector quantized variational autoencoder (https://arxiv.org/abs/1711.00937)', 'readme': '# Vector Quantized Variational Autoencoder\n\nThis is a PyTorch implementation of the vector quantized variational autoencoder (https://arxiv.org/abs/1711.00937). \n\nYou can find the author\'s [original implementation in Tensorflow here](https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py) with [an example you can run in a Jupyter notebook](https://github.com/deepmind/sonnet/blob/master/sonnet/examples/vqvae_example.ipynb).\n\n## Installing Dependencies\n\nTo install dependencies, create a conda or virtual environment with Python 3 and then run `pip install -r requirements.txt`. \n\n## Running the VQ VAE\n\nTo run the VQ-VAE simply run `python3 main.py`. Make sure to include the `-save` flag if you want to save your model. You can also add parameters in the command line. The default values are specified below:\n\n```python\nparser.add_argument("--batch_size", type=int, default=32)\nparser.add_argument("--n_updates", type=int, default=5000)\nparser.add_argument("--n_hiddens", type=int, default=128)\nparser.add_argument("--n_residual_hiddens", type=int, default=32)\nparser.add_argument("--n_residual_layers", type=int, default=2)\nparser.add_argument("--embedding_dim", type=int, default=64)\nparser.add_argument("--n_embeddings", type=int, default=512)\nparser.add_argument("--beta", type=float, default=.25)\nparser.add_argument("--learning_rate", type=float, default=3e-4)\nparser.add_argument("--log_interval", type=int, default=50)\n```\n\n## Models\n\nThe VQ VAE has the following fundamental model components:\n\n1. An `Encoder` class which defines the map `x -> z_e`\n2. A `VectorQuantizer` class which transform the encoder output into a discrete one-hot vector that is the index of the closest embedding vector `z_e -> z_q`\n3. A `Decoder` class which defines the map `z_q -> x_hat` and reconstructs the original image\n\nThe Encoder / Decoder classes are convolutional and inverse convolutional stacks, which include Residual blocks in their architecture [see ResNet paper](https://arxiv.org/abs/1512.03385). The residual models are defined by the `ResidualLayer` and `ResidualStack` classes.\n\nThese components are organized in the following folder structure:\n\n```\nmodels/\n    - decoder.py -> Decoder\n    - encoder.py -> Encoder\n    - quantizer.py -> VectorQuantizer\n    - residual.py -> ResidualLayer, ResidualStack\n    - vqvae.py -> VQVAE\n```\n\n## PixelCNN - Sampling from the VQ VAE latent space \n\nTo sample from the latent space, we fit a PixelCNN over the latent pixel values `z_ij`. The trick here is recognizing that the VQ VAE maps an image to a latent space that has the same structure as a 1 channel image. For example, if you run the default VQ VAE parameters you\'ll RGB map images of shape `(32,32,3)` to a latent space with shape `(8,8,1)`, which is equivalent to an 8x8 grayscale image. Therefore, you can use a PixelCNN to fit a distribution over the "pixel" values of the 8x8 1-channel latent space.\n\nTo train the PixelCNN on latent representations, you first need to follow these steps:\n\n1. Train the VQ VAE on your dataset of choice\n2. Use saved VQ VAE parameters to encode your dataset and save discrete latent space representations with `np.save` API. In the `quantizer.py` this is the `min_encoding_indices` variable. \n3. Specify path to your saved latent space dataset in `utils.load_latent_block` function.\n4. Run the PixelCNN script\n\nTo run the PixelCNN, simply type \n\n`python pixelcnn/gated_pixelcnn.py`\n\nas well as any parameters (see the argparse statements). The default dataset is `LATENT_BLOCK` which will only work if you have trained your VQ VAE and saved the latent representations.\n', 'contents': "['.gitignore', 'README.md', '__init__.py', 'datasets', 'main.py', 'models', 'pixelcnn', 'requirements.txt', 'utils.py', 'visualization.ipynb']", 'stars': 11, 'watchers': 11, 'forks': 3, 'deprepos': 'zero', 'deppacks': 'zero'}