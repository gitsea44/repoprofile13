{'dependent': 'kerasrl', 'repo': '/miroblog/tf_deep_rl_trader', 'language': 'Python', 'description': 'Trading Environment(OpenAI Gym) + PPO(TensorForce)', 'readme': '# Deep RL Trader + PPO Agent Implemented using Tensorforce\n\nThis repo contains \n1. Trading environment(OpenAI Gym) + Wrapper for Tensorforce Env \n2. PPO(Proximal Policy Optimization) Agent (https://arxiv.org/abs/1707.06347)\nAgent is implemented using `tensorforce`(https://github.com/reinforceio/tensorforce)     \n  \nAgent is expected to learn useful action sequences to maximize profit in a given environment.  \nEnvironment limits agent to either buy, sell, hold stock(coin) at each step.  \nIf an agent decides to take a   \n* LONG position it will initiate sequence of action such as `buy- hold- hold- sell`    \n* for a SHORT position vice versa (e.g.) `sell - hold -hold -buy`.    \n\nOnly a single position can be opened per trade. \n* Thus invalid action sequence like `buy - buy` will be considered `buy- hold`.   \n* Default transaction fee is : 0.0005  \n\nReward is given\n* when the position is closed or\n* an episode is finished.   \n  \nThis type of sparse reward granting scheme takes longer to train but is most successful at learning long term dependencies.  \n\nAgent decides optimal action by observing its environment.  \n* Trading environment will emit features derived from ohlcv-candles(the window size can be configured). \n* Thus, input given to the agent is of the shape `(window_size, n_features)`.  \n\nWith some modification it can easily be applied to stocks, futures or foregin exchange as well.\n\n[Visualization](https://github.com/miroblog/tf_deep_rl_trader/blob/master/visualize_info.ipynb) / [Main](https://github.com/miroblog/tf_deep_rl_trader/blob/master/ppo_trader.py) / [Environment](https://github.com/miroblog/tf_deep_rl_trader/blob/master/env/TFTraderEnv.py)\n\nSample data provided is 5min ohlcv candle fetched from bitmex.\n* train : `\'./data/train/` 70000\n* test : `\'./data/train/` 16000\n\n### Prerequisites\n\nkeras-rl, numpy, tensorflow ... etc\n\n```python\npip install -r requirements.txt\n\n```\n\n## Getting Started \n\n### Create Environment & Agent\n```python\n# create environment\n# OPTIONS\n# create environment for train and test\nPATH_TRAIN = "./data/train/"\nPATH_TEST = "./data/test/"\nTIMESTEP = 30  # window size\nenvironment = create_btc_env(window_size=TIMESTEP, path=PATH_TRAIN, train=True)\ntest_environment = create_btc_env(window_size=TIMESTEP, path=PATH_TEST, train=False)\n\n# create spec for network and baseline\nnetwork_spec = create_network_spec() # json format\nbaseline_spec = create_baseline_spec()\n\n# create agent\nagent = PPOAgent(\n    discount=0.9999,\n    states=environment.states,\n    actions=environment.actions,\n    network=network_spec,\n    # Agent\n    states_preprocessing=None,\n    actions_exploration=None,\n    reward_preprocessing=None,\n    # MemoryModel\n    update_mode=dict(\n        unit=\'timesteps\',  # \'episodes\',\n        # 10 episodes per update\n        batch_size=32,\n        # # Every 10 episodes\n        frequency=10\n    ),\n    memory=dict(\n        type=\'latest\',\n        include_next_states=False,\n        capacity=50000\n    ),\n    # DistributionModel\n    distributions=None,\n    entropy_regularization=0.0,  # None\n    # PGModel\n\n    baseline_mode=\'states\',\n    baseline=dict(type=\'custom\', network=baseline_spec),\n    baseline_optimizer=dict(\n        type=\'multi_step\',\n        optimizer=dict(\n            type=\'adam\',\n            learning_rate=(1e-4)  # 3e-4\n        ),\n        num_steps=5\n    ),\n    gae_lambda=0,  # 0\n    # PGLRModel\n    likelihood_ratio_clipping=0.2,\n    # PPOAgent\n    step_optimizer=dict(\n        type=\'adam\',\n        learning_rate=(1e-4)  # 1e-4\n    ),\n    subsampling_fraction=0.2,  # 0.1\n    optimization_steps=10,\n    execution=dict(\n        type=\'single\',\n        session_config=None,\n        distributed_spec=None\n    )\n)\n\n```\n\n### Train and Validate\n```python\n    train_runner = Runner(agent=agent, environment=environment)\n    test_runner = Runner(\n        agent=agent,\n        environment=test_environment,\n    )\n\n    train_runner.run(episodes=100, max_episode_timesteps=16000, episode_finished=episode_finished)\n    print("Learning finished. Total episodes: {ep}. Average reward of last 100 episodes: {ar}.".format(\n        ep=train_runner.episode,\n        ar=np.mean(train_runner.episode_rewards[-100:]))\n    )\n\n    test_runner.run(num_episodes=1, deterministic=True, testing=True, episode_finished=print_simple_log)\n```\n\n### Configuring Agent\n```python\n## you can stack layers using blocks provided by tensorforce or define ur own...\ndef create_network_spec():\n    network_spec = [\n        {\n            "type": "flatten"\n        },\n        dict(type=\'dense\', size=32, activation=\'relu\'),\n        dict(type=\'dense\', size=32, activation=\'relu\'),\n        dict(type=\'internal_lstm\', size=32),\n    ]\n    return network_spec\n\ndef create_baseline_spec():\n    baseline_spec = [\n        {\n            "type": "lstm",\n            "size": 32,\n        },\n        dict(type=\'dense\', size=32, activation=\'relu\'),\n        dict(type=\'dense\', size=32, activation=\'relu\'),\n    ]\n    return baseline_spec\n```\n\n### Running \n[Verbose] While training or testing, \n* environment will print out (current_tick , # Long, # Short, Portfolio)\n  \n[Portfolio]  \n* initial portfolio starts with 100*10000(krw-won)     \n* reflects change in portfolio value if the agent had invested 100% of its balance every time it opened a position.       \n  \n[Reward] \n* simply pct earning per trade.    \n\n### Inital Result\n\n#### Portfolio Value Change, Max DrawDown period in Red\n![trade](https://github.com/miroblog/tf_deep_rl_trader/blob/master/portfolio_change.png)  \n\n* portfolio value 1000000 -> 1586872.1775 in 56 days\n\nNot bad but the agent definitely needs more\n* training data and \n* degree of freedom  (larger network)\n  \nBeaware of overfitting ! \n\n## Authors\n\n* **Lee Hankyol** - *Initial work* - [tf_deep_rl_trader](https://github.com/miroblog/tf_deep_rl_trader)\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details\n', 'contents': "['.gitattributes', '.idea', '.ipynb_checkpoints', 'Readme.md', '__pycache__', 'data', 'env', 'portfolio_change.png', 'ppo_trader.py', 'process_data.py', 'requirements.txt', 'ta.py', 'visualize_info.ipynb']", 'stars': 69, 'watchers': 69, 'forks': 24, 'deprepos': 'zero', 'deppacks': 'zero'}