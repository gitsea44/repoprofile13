{'dependent': 'kerasrl', 'repo': '/andriusbern/nao_rl', 'language': 'Python', 'description': 'A Reinforcement Learning framework for the NAO robot', 'readme': '# nao_rl  -  Reinforcement Learning Package for the *Nao* Robot.\nThis python package integrates *V-REP* robot simulation software, base libraries for NAO robot control along with reinforcement learning algorithms for solving custom or any *OpenAI-gym*-based learning environments.\n\n## Features:\n1. Parallelized Proximal Policy Optimization (PPO) and Asynchronous Advantage Actor-Critic (A3C) for training agents.\n2. Custom OpenAI-gym-based API for controlling *V-REP* that makes it easy to create new learning tasks and environments (50 - 100 LOC)\n3. Learned policies can be transferred back to the real robot, or learning can be done online (not recommended).\n5. Custom learning environments for the NAO robot:\n   \n\n  |                      **1. Balancing / Learning a bipedal gait**                      |                              **2. Object tracking**                               |\n  | :----------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------: |\n  |                <img src="assets/ezgif.com-gif-maker.gif" width="600">                |                    <img src="assets/untitled.gif" width="500">                    |\n  | The goal is to keep an upright position without falling or learn how to move forward | The goal is to keep the object within the visual field by moving the head motors. |\n\n\n\n# Requirements\n\n### **Base** (for learning in simulated environments with virtual NAO):\n1. [VREP v3.4.0](http://coppeliarobotics.com/files/V-REP_PRO_EDU_V3_4_0_Linux.tar.gz) - Robot simulation software by Copellia Robotics\n2. *Python 2.7* and *python-virtualenv*. \n3. *tensorflow, gym, numpy, opencv-python*\n    \n\n### **Optional** (if you have access to the NAO robot):\n1. [Choregraphe Suite v2.1.2](https://community.ald.softbankrobotics.com/en/dl/ZmllbGRfY29sbGVjdGlvbl9pdGVtLTQyNS1maWVsZF9zb2Z0X2RsX2V4dGVybmFsX2xpbmstMC01MzY2MjU%3D?width=500&height=auto) - for creating a virtual NAO (**requires registering**) (By default installed in */opt/Aldebaran/Choregraphe Suite 2.1*)\n\n2. [Python NAOqi SDK v2.1.2](https://community.ald.softbankrobotics.com/en/dl/ZmllbGRfY29sbGVjdGlvbl9pdGVtLTQ1My1maWVsZF9zb2Z0X2RsX2V4dGVybmFsX2xpbmstMC1lZWFmOGU%3D?width=500&height=auto) - Libraries provided by Softbank robotics for NAO control (**requires registering**)\n\n# Installation\n*(Tested on Ubuntu 18.04)*\n\n**1. Clone the repository**\n   \n```\ngit clone https://github.com/andriusbern/nao_rl\ncd nao-rl\n```\n**2. Create and activate the virtual environment**\n   \n```\nvirtualenv env\nsource env/bin/activate\n```\n\n**3. Install the package and the required libraries**\n```\npython setup.py install\n```\nYou will be prompted to enter the path to your V-Rep installation directory\n\n\n# Testing the environments\nTo try the environments out (V-Rep will be launched with the appropriate scene and agent loaded, actions will be sampled randomly):\n```\nimport nao_rl\nenv = nao_rl.make(\'env_name\')\nenv.run()\n```\nWhere \'env_name\' corresponds to one of the following available environments:\n1. NaoTracking  - tracking an object using the camera information\n2. NaoBalancing - keeping upright balance\n3. NaoWalking   - learning a bipedal gait\n\n# Training\n\nTo train the agents in these environments you can use built-in RL algorithms:\n\n```\npython train.py NaoTracking a3c 0\n```\n\n| ![](assets/live_plot.gif) | \n|:--:| \n|*Live plotting of training results.* *Sped up by 40x (enabled with flag \'-p\').*|\n\n### Positional arguments:\n1. Environment name:      name one of **nao_rl** or **gym** environments\n2. Training algorithm: \n   1. *--a3c*    - Asynchronous Advantage Actor-Critic or \n   2. *--ppo*    - Proximal Policy Optimization\n3. Rendering mode:\n   1. [0] - Do not render\n   2. [1] - Render the first worker\n   3. [2] - Render all workers\n\nTo find out more about additional command line arguments:\n```\npython train.py -h\n```\n\nThe training session can be interrupted at any time and the model is going to be saved and can be loaded later.\n\n# Testing trained models\n\nTo test trained models:\n```\npython test.py trained_models/filename.cpkt \n```\nAdd *-r* flag to run the trained policy on the real NAO (can be dangerous). It is recommended to set low fps for the environment e.g. (the robot will perform actions slowly):\n```\npython test.py trained_models/filename.cpkt -r -fps 2\n```\n\n\n\n', 'contents': "['.gitignore', 'README.md', 'assets', 'nao_rl', 'setup.py', 'vrep.sh']", 'stars': 8, 'watchers': 8, 'forks': 1, 'deprepos': 'zero', 'deppacks': 'zero'}